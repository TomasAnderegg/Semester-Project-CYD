# Post-Mortem: Hard Negative Mining n'a pas am√©lior√© les r√©sultats

## üìä R√©sultats observ√©s

| M√©trique | Focal seul | Focal + Hard (ratio=0.5, temp=0.1) | Changement |
|----------|------------|-------------------------------------|------------|
| Precision@1000 | 0.3% (3/1000) | 0.1% (1/1000) | ‚ùå **-67%** |
| Recall@1000 | 5.77% | 1.92% | ‚ùå -67% |
| Rank m√©dian | 5,623 | 5,102 | ‚úÖ +9% |
| vs Random | 9.85x | 3.28x | ‚ùå -67% |

**Conclusion**: Hard Negative Mining a **d√©grad√©** les performances au lieu de les am√©liorer.

## üîç Analyse des causes

### Cause 1: Features statiques inad√©quates

**Probl√®me**: Le sampler utilise `node_features` (features brutes) pour calculer la similarit√©:

```python
# train_self_supervised.py:315-322
node_features_np = node_features  # Features STATIQUES
negatives_batch = hard_neg_sampler.sample(
    embeddings=node_features_np,  # ‚ö†Ô∏è Pas les embeddings temporels!
    ...
)
```

**Pourquoi c'est un probl√®me**:
- `node_features` sont **statiques**: secteur, taille, ann√©e fondation, etc.
- Ne capturent **pas** la dynamique temporelle des investissements
- Ne capturent **pas** les patterns de co-investissement
- Ne capturent **pas** les pr√©f√©rences subtiles des investisseurs

**Exemple concret**:
```
Company: Startup AI dans la sant√© (features: [sector=AI, stage=seed, ...])

Hard negatives s√©lectionn√©s par similarit√© de features:
- Investor A: Sp√©cialis√© AI mais n'investit QUE dans fintech (pas sant√©)
- Investor B: Sp√©cialis√© sant√© mais n'investit QUE dans late-stage (pas seed)

Ces investisseurs sont "similaires" sur papier mais ne sont PAS des hard negatives pertinents!
```

### Cause 2: Hyperparam√®tres trop agressifs

**Configuration test√©e**:
- `--hard_neg_ratio 0.5`: 50% de hard negatives
- `--hard_neg_temperature 0.1`: Tr√®s agressif (s√©lectionne top similaires)

**Probl√®me**:
- Avec 50% de n√©gatifs "mal choisis", le mod√®le apprend des patterns incorrects
- Temperature 0.1 s√©lectionne presque exclusivement les plus similaires, sans diversit√©

### Cause 3: Gap entre training et evaluation

**Training**: Negative sampling sur un seul n√©gatif par positif
```python
n_negatives=1  # Seulement 1 n√©gatif par edge
```

**Evaluation temporelle**: Complete ranking sur 170,742 paires
```
199 companies √ó 858 investors = 170,742 paires
```

**Probl√®me**: Le mod√®le apprend √† distinguer 1 n√©gatif (hard ou random) mais doit ensuite ranker 170K paires. Le gap est √©norme.

## üí° Pourquoi le rang m√©dian s'est am√©lior√© mais pas Precision@1000?

**Observation paradoxale**:
- Rang m√©dian: 5,623 ‚Üí 5,102 (‚úÖ am√©lioration +9%)
- Precision@1000: 0.3% ‚Üí 0.1% (‚ùå d√©gradation -67%)

**Explication**:
1. Hard negatives a rendu le mod√®le **plus conservateur**
2. Le mod√®le assigne des probabilit√©s **plus uniformes** (range plus √©troit)
3. R√©sultat:
   - Quelques vrais liens ont mont√© dans le ranking (m√©diane am√©liore)
   - Mais beaucoup d'autres ont descendu (moins de hits dans top-1000)
   - La **variance** du ranking a augment√©

**Preuve dans les probabilit√©s**:
- Focal seul: m√©diane global 0.243, m√©diane vrais 0.378, **gap = 0.135**
- Focal+Hard: m√©diane global 0.334, m√©diane vrais 0.453, **gap = 0.119**

Le gap a **r√©duit** (0.135 ‚Üí 0.119), ce qui signifie que le mod√®le discrimine **moins bien**.

## ‚úÖ Ce qui a fonctionn√© (√† garder)

### Focal Loss seul

**Meilleurs r√©sultats observ√©s**:
- Precision@1000: **0.3%** (3/1000)
- Recall@1000: **5.77%** (3/52)
- vs Random: **9.85x** meilleur que baseline
- Rang m√©dian: **5,623** (top 3.3%)

**Pourquoi √ßa marche**:
- Focal Loss s'attaque au **vrai** probl√®me: d√©s√©quilibre de classes (0.03% positifs)
- R√©duit l'importance des n√©gatifs faciles
- Force le mod√®le √† se concentrer sur les exemples difficiles
- **Compatible** avec random sampling (pas besoin de features pour identifier hard negatives)

## üö´ Ce qui n'a PAS fonctionn√© (√† √©viter)

### Hard Negative Mining (avec features statiques)

**Pourquoi √ßa n'a pas march√©**:
1. Features statiques ne capturent pas la dynamique temporelle
2. "Similarit√©" bas√©e sur features != "difficult√©" pour le mod√®le
3. Gap √©norme entre training (1 n√©gatif) et evaluation (170K paires)

## üéØ Recommandations

### Court terme: Utiliser Focal Loss seul

**Commande recommand√©e** (best config observ√©e):
```bash
python train_self_supervised.py \
  --use_memory \
  --prefix tgn-focal-final \
  --n_epoch 50 \
  --patience 10 \
  --lr 1e-4 \
  --node_dim 200 \
  --time_dim 200 \
  --memory_dim 200 \
  --message_dim 200 \
  --use_focal_loss \
  --focal_alpha 0.25 \
  --focal_gamma 2.0 \
  --n_runs 1
```

**R√©sultats attendus**:
- Precision@1000: ~0.3% (stable, reproductible)
- vs Random: ~10x meilleur que baseline
- Suffisant pour TechRank (top-1000 contient ~3 vrais liens)

### Moyen terme: Am√©liorer Hard Negative Mining

Si tu veux r√©essayer Hard Negatives, il faut **corriger** l'impl√©mentation:

**Option A: Utiliser embeddings temporels** (au lieu de features statiques)

```python
# Au lieu de:
negatives_batch = hard_neg_sampler.sample(
    embeddings=node_features,  # Static features
    ...
)

# Faire:
# 1. Calculer embeddings temporels pour tous les nodes
with torch.no_grad():
    temporal_embeddings = tgn.compute_all_node_embeddings(current_timestamp)

# 2. Utiliser ces embeddings pour hard sampling
negatives_batch = hard_neg_sampler.sample(
    embeddings=temporal_embeddings.cpu().numpy(),  # Temporal embeddings!
    ...
)
```

**Probl√®me**: Tr√®s co√ªteux computationnellement (calculer embeddings pour tous les nodes √† chaque batch)

**Option B: Graph-based hard negatives**

Au lieu de similarit√© de features, utiliser la **structure du graphe**:

```python
# Hard negatives = nodes √† distance 2-3 dans le graphe
# (amis d'amis, mais pas directement connect√©s)

def sample_graph_hard_negatives(src, adjacency, k=2):
    """Sample nodes at distance 2-k from src"""
    # 1-hop neighbors
    neighbors_1hop = adjacency[src]

    # 2-hop neighbors (friends of friends)
    neighbors_2hop = set()
    for neighbor in neighbors_1hop:
        neighbors_2hop.update(adjacency[neighbor])

    # Remove 1-hop neighbors (they're positives)
    hard_negatives = neighbors_2hop - neighbors_1hop - {src}

    return random.sample(hard_negatives, min(len(hard_negatives), n_samples))
```

**Avantage**: Capture la structure du graphe sans besoin de features

**Option C: Tester hyperparam√®tres plus conservateurs**

Avant d'abandonner compl√®tement, tester:
- `--hard_neg_ratio 0.2` (20% hard, 80% random)
- `--hard_neg_temperature 1.0` (moins agressif)

Voir [test_hard_neg_hyperparams.sh](test_hard_neg_hyperparams.sh)

### Long terme: Autres approches

Si Focal Loss + ajustements ne suffisent pas:

1. **Curriculum Learning**: Augmenter progressivement la difficult√©
   - Epochs 1-10: Random negatives
   - Epochs 11-30: 20% hard negatives
   - Epochs 31-50: 50% hard negatives

2. **Multi-task Learning**: Entra√Æner sur plusieurs t√¢ches simultan√©ment
   - T√¢che 1: Link prediction (comme maintenant)
   - T√¢che 2: Node classification (pr√©dire type d'investisseur)
   - T√¢che 3: Temporal prediction (pr√©dire d√©lai avant investissement)

3. **Ensemble Methods**: Combiner plusieurs mod√®les
   - Mod√®le 1: Focal Loss
   - Mod√®le 2: Weighted BCE
   - Mod√®le 3: Different architecture
   - Pr√©diction finale: moyenne/vote des 3

4. **Graph Augmentation**: Enrichir les features avec structure du graphe
   - Node centrality (PageRank, Betweenness)
   - Community detection (modules de co-investissement)
   - Temporal features (activit√© r√©cente, tendances)

## üìö Lessons Learned

1. **Mesurer avant d'optimiser**: Focal Loss a bien march√© car il s'attaquait au probl√®me identifi√© (class imbalance)

2. **Features matter**: Hard negatives bas√©s sur features inad√©quates peuvent empirer les r√©sultats

3. **Hyperparam√®tres matter**: ratio=0.5 √©tait peut-√™tre trop agressif pour un premier essai

4. **Gap training/eval**: Toujours se rappeler de l'√©cart entre comment on entra√Æne (1 n√©gatif) et comment on √©value (170K paires)

5. **It√©ration progressive**: Tester une technique √† la fois, mesurer, ajuster avant d'ajouter la suivante

## üéì Conclusion

Hard Negative Mining est une technique puissante **en th√©orie**, mais son efficacit√© d√©pend fortement de:
- **Qualit√© des features** utilis√©es pour identifier les hard negatives
- **Hyperparam√®tres** (ratio, temperature)
- **Ad√©quation** avec la t√¢che finale (gap training/eval)

Dans notre cas:
- ‚úÖ Focal Loss fonctionne bien (9.85x vs random)
- ‚ùå Hard Negatives (avec features statiques) d√©grade les performances (-67%)
- üí° Rester sur Focal Loss seul pour maintenant
- üî¨ Am√©liorer Hard Negatives reste une piste future si on a les bonnes features
