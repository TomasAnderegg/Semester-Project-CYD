{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create classes\n",
    "\n",
    "This notebook is the first one that should be run in order to start the TeckRank. Here, we upload the data and we create the classes for companies and technologies. \n",
    "\n",
    "Then we save the results as two dictionaries (company_name:class_company and tech_name:class_tech), which contain all the needed information for the nest steps\n",
    "\n",
    "### Table of contents:\n",
    "\n",
    "* [Download data from CSV](#down)\n",
    "* [Data cleaning](#cleaning)\n",
    "* [Select companies in cybersecurity](#cyber)\n",
    "* [Create graph and dictionaries](#create_graph)\n",
    "* [Save graph and dictionaries](#save)\n",
    "* [Quick loop](#loop0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_cybersecurity = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import arrow\n",
    "import ipynb \n",
    "import os.path\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "import random\n",
    "import operator\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from networkx.algorithms.bipartite.matrix import biadjacency_matrix\n",
    "from networkx.algorithms import bipartite\n",
    "from importlib import reload\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions from py file \n",
    "\n",
    "import functions.fun\n",
    "reload(functions.fun)\n",
    "from functions.fun import CB_data_cleaning, df_from_api_CB, extract_nodes, extract_data_from_column, field_extraction\n",
    "from functions.fun import nx_dip_graph_from_pandas, plot_bipartite_graph, filter_dict, check_desc\n",
    "from functions.fun import extract_classes_company_tech, degree_bip, insert_data_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions from py file \n",
    "\n",
    "import functions.fun_meth_reflections\n",
    "reload(functions.fun_meth_reflections)\n",
    "from functions.fun_meth_reflections import zero_order_score, Gct_beta, Gtc_alpha, make_G_hat, next_order_score, generator_order_w\n",
    "from functions.fun_meth_reflections import M_test_triangular, w_stream, find_convergence, rank_df_class, w_star_analytic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'classes' from 'c:\\\\Users\\\\tjga9\\\\Documents\\\\Tomas\\\\EPFL\\\\MA3\\\\CYD PDS\\\\Code\\\\TechRank\\\\5-TechRank-main\\\\5-TechRank-main\\\\classes.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import classes \n",
    "\n",
    "import classes\n",
    "reload(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data from CSV <a class=\"anchor\" id=\"down\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Juste après le chargement : 50 lignes\n"
     ]
    }
   ],
   "source": [
    "df_start = pd.read_csv(r\"C:\\Users\\tjga9\\Documents\\Tomas\\EPFL\\MA3\\CYD PDS\\Code\\TechRank\\5-TechRank-main\\5-TechRank-main\\data\\sample CB data\\organizations.csv\")\n",
    "\n",
    "df_start.head()\n",
    "# 1. Vérifie immédiatement après le chargement des données\n",
    "print(f\"Juste après le chargement : {len(df_start)} lignes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uuid', 'name', 'type', 'permalink', 'cb_url', 'rank', 'created_at',\n",
       "       'updated_at', 'legal_name', 'roles', 'domain', 'homepage_url',\n",
       "       'country_code', 'state_code', 'region', 'city', 'address',\n",
       "       'postal_code', 'status', 'short_description', 'category_list',\n",
       "       'category_groups_list', 'num_funding_rounds', 'total_funding_usd',\n",
       "       'total_funding', 'total_funding_currency_code', 'founded_on',\n",
       "       'last_funding_on', 'closed_on', 'employee_count', 'email', 'phone',\n",
       "       'facebook_url', 'linkedin_url', 'twitter_url', 'logo_url', 'alias1',\n",
       "       'alias2', 'alias3', 'primary_role', 'num_exits', 'revenue_range'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_start.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning <a class=\"anchor\" id=\"cleaning\"></a>\n",
    "\n",
    "We decide to use as key the name. For the future, it would be better to use the uuid\n",
    "\n",
    "- `df_start`: dataset before cleaning\n",
    "- `df` : datsety after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create the lists needed as input in the function to clean the data\n",
    "\n",
    "to_drop = [\n",
    "    'type',\n",
    "    'permalink',\n",
    "    'cb_url',   \n",
    "    'created_at',\n",
    "    'domain',\n",
    "    'address',\n",
    "    'state_code',\n",
    "    'updated_at',\n",
    "    'legal_name',\n",
    "    'roles',\n",
    "    'postal_code',\n",
    "    'homepage_url',\n",
    "    'num_funding_rounds',\n",
    "    'total_funding_currency_code',\n",
    "    'phone',\n",
    "    'email',\n",
    "    'num_exits',\n",
    "    'alias2',\n",
    "    'alias3',\n",
    "    'num_exits',\n",
    "    'logo_url',\n",
    "    'alias1',\n",
    "    'last_funding_on',\n",
    "    'twitter_url',\n",
    "    'facebook_url'\n",
    "]\n",
    "\n",
    "# to_rename = { 'category_groups_list': 'category_groups' }\n",
    "to_rename = { 'category_list': 'category_groups' }\n",
    "\n",
    "drop_if_nan = [\n",
    "    'category_groups',\n",
    "    'rank',\n",
    "    'short_description'\n",
    "]\n",
    "\n",
    "to_check_double = {}\n",
    "\n",
    "sort_by = \"rank\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Juste après le chargement : 47 lignes\n"
     ]
    }
   ],
   "source": [
    "# clean data: from df_start to df\n",
    "df = CB_data_cleaning(df_start, to_drop, to_rename, to_check_double, drop_if_nan, sort_by)\n",
    "print(f\"Juste après le chargement : {len(df)} lignes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>name</th>\n",
       "      <th>rank</th>\n",
       "      <th>country_code</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>status</th>\n",
       "      <th>short_description</th>\n",
       "      <th>category_groups</th>\n",
       "      <th>category_groups_list</th>\n",
       "      <th>total_funding_usd</th>\n",
       "      <th>total_funding</th>\n",
       "      <th>founded_on</th>\n",
       "      <th>closed_on</th>\n",
       "      <th>employee_count</th>\n",
       "      <th>linkedin_url</th>\n",
       "      <th>primary_role</th>\n",
       "      <th>revenue_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0fbfb7ac-4015-1561-6d42-ec9c4a87a324</td>\n",
       "      <td>Paladina Health</td>\n",
       "      <td>7675.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>Denver</td>\n",
       "      <td>acquired</td>\n",
       "      <td>Paladina Health is an innovative employer-spon...</td>\n",
       "      <td>Health Care,Hospital,Medical,Personal Health</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>165000000.0</td>\n",
       "      <td>1.650000e+08</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11-50</td>\n",
       "      <td>https://www.linkedin.com/company/paladina-health</td>\n",
       "      <td>company</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>17818d55-4f93-94b1-6b80-575a7cdc5878</td>\n",
       "      <td>Critical Force</td>\n",
       "      <td>13752.0</td>\n",
       "      <td>FIN</td>\n",
       "      <td>Oulu</td>\n",
       "      <td>Kajaani</td>\n",
       "      <td>operating</td>\n",
       "      <td>Critical Force is a Finnish video game company</td>\n",
       "      <td>Gaming,Mobile Devices,Video Games</td>\n",
       "      <td>Consumer Electronics,Gaming,Hardware,Mobile</td>\n",
       "      <td>10760303.0</td>\n",
       "      <td>1.076030e+07</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11-50</td>\n",
       "      <td>https://www.linkedin.com/company/critical-forc...</td>\n",
       "      <td>company</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>daa9dd72-86f3-bafd-c9ec-88fb18eeed2a</td>\n",
       "      <td>ALBERT</td>\n",
       "      <td>15806.0</td>\n",
       "      <td>JPN</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>ipo</td>\n",
       "      <td>ALBERT offers businesses with analytics and co...</td>\n",
       "      <td>Analytics,Database</td>\n",
       "      <td>Data and Analytics,Software</td>\n",
       "      <td>21756468.0</td>\n",
       "      <td>2.409890e+09</td>\n",
       "      <td>2005-07-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>company</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0cbe819b-a9c0-d059-b5c3-7e7a859c4ec7</td>\n",
       "      <td>Trainline Europe</td>\n",
       "      <td>19943.0</td>\n",
       "      <td>FRA</td>\n",
       "      <td>Ile-de-France</td>\n",
       "      <td>Paris</td>\n",
       "      <td>acquired</td>\n",
       "      <td>Trainline (formerly Captain Train) sells train...</td>\n",
       "      <td>Internet,Ticketing,Travel</td>\n",
       "      <td>Events,Internet Services,Media and Entertainme...</td>\n",
       "      <td>11984031.0</td>\n",
       "      <td>9.400000e+06</td>\n",
       "      <td>2009-02-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11-50</td>\n",
       "      <td>https://www.linkedin.com/showcase/trainline-eu/</td>\n",
       "      <td>company</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b612bc69-c1dd-a92e-a349-a5b767922df8</td>\n",
       "      <td>Applied BioCode</td>\n",
       "      <td>37317.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>California</td>\n",
       "      <td>Santa Fe Springs</td>\n",
       "      <td>operating</td>\n",
       "      <td>Applied BioCode commercializes a multiplexing ...</td>\n",
       "      <td>Biotechnology,Genetics,Health Diagnostics,Life...</td>\n",
       "      <td>Biotechnology,Health Care,Science and Engineering</td>\n",
       "      <td>17505680.0</td>\n",
       "      <td>1.750568e+07</td>\n",
       "      <td>2008-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11-50</td>\n",
       "      <td>https://www.linkedin.com/company/appliedbiocode</td>\n",
       "      <td>company</td>\n",
       "      <td>$1M to $10M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    uuid              name     rank  \\\n",
       "3   0fbfb7ac-4015-1561-6d42-ec9c4a87a324   Paladina Health   7675.0   \n",
       "46  17818d55-4f93-94b1-6b80-575a7cdc5878    Critical Force  13752.0   \n",
       "38  daa9dd72-86f3-bafd-c9ec-88fb18eeed2a            ALBERT  15806.0   \n",
       "8   0cbe819b-a9c0-d059-b5c3-7e7a859c4ec7  Trainline Europe  19943.0   \n",
       "5   b612bc69-c1dd-a92e-a349-a5b767922df8   Applied BioCode  37317.0   \n",
       "\n",
       "   country_code         region              city     status  \\\n",
       "3           USA       Colorado            Denver   acquired   \n",
       "46          FIN           Oulu           Kajaani  operating   \n",
       "38          JPN          Tokyo             Tokyo        ipo   \n",
       "8           FRA  Ile-de-France             Paris   acquired   \n",
       "5           USA     California  Santa Fe Springs  operating   \n",
       "\n",
       "                                    short_description  \\\n",
       "3   Paladina Health is an innovative employer-spon...   \n",
       "46     Critical Force is a Finnish video game company   \n",
       "38  ALBERT offers businesses with analytics and co...   \n",
       "8   Trainline (formerly Captain Train) sells train...   \n",
       "5   Applied BioCode commercializes a multiplexing ...   \n",
       "\n",
       "                                      category_groups  \\\n",
       "3        Health Care,Hospital,Medical,Personal Health   \n",
       "46                  Gaming,Mobile Devices,Video Games   \n",
       "38                                 Analytics,Database   \n",
       "8                           Internet,Ticketing,Travel   \n",
       "5   Biotechnology,Genetics,Health Diagnostics,Life...   \n",
       "\n",
       "                                 category_groups_list  total_funding_usd  \\\n",
       "3                                         Health Care        165000000.0   \n",
       "46        Consumer Electronics,Gaming,Hardware,Mobile         10760303.0   \n",
       "38                        Data and Analytics,Software         21756468.0   \n",
       "8   Events,Internet Services,Media and Entertainme...         11984031.0   \n",
       "5   Biotechnology,Health Care,Science and Engineering         17505680.0   \n",
       "\n",
       "    total_funding  founded_on  closed_on employee_count  \\\n",
       "3    1.650000e+08  2010-01-01        NaN          11-50   \n",
       "46   1.076030e+07  2012-01-01        NaN          11-50   \n",
       "38   2.409890e+09  2005-07-01        NaN        unknown   \n",
       "8    9.400000e+06  2009-02-08        NaN          11-50   \n",
       "5    1.750568e+07  2008-01-01        NaN          11-50   \n",
       "\n",
       "                                         linkedin_url primary_role  \\\n",
       "3    https://www.linkedin.com/company/paladina-health      company   \n",
       "46  https://www.linkedin.com/company/critical-forc...      company   \n",
       "38                                                NaN      company   \n",
       "8     https://www.linkedin.com/showcase/trainline-eu/      company   \n",
       "5     https://www.linkedin.com/company/appliedbiocode      company   \n",
       "\n",
       "   revenue_range  \n",
       "3            NaN  \n",
       "46           NaN  \n",
       "38           NaN  \n",
       "8            NaN  \n",
       "5    $1M to $10M  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show cleaned dataset\n",
    "# .head() shows the first 5 rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Juste après le chargement : 47 lignes\n"
     ]
    }
   ],
   "source": [
    "df.columns\n",
    "print(f\"Juste après le chargement : {len(df)} lignes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47c409e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVANT la conversion :\n",
      "Type : <class 'str'>\n",
      "Exemple : Health Care,Hospital,Medical,Personal Health\n",
      "\n",
      "✓ Conversion effectuée\n",
      "\n",
      "APRÈS la conversion :\n",
      "Type : <class 'list'>\n",
      "Exemple : ['Health Care', 'Hospital', 'Medical', 'Personal Health']\n",
      "\n",
      "Toutes les valeurs sont des listes : True\n",
      "Nombre de lignes après conversion : 47\n"
     ]
    }
   ],
   "source": [
    "# Vérifier AVANT la conversion\n",
    "print(\"AVANT la conversion :\")\n",
    "print(f\"Type : {type(df['category_groups'].iloc[0])}\")\n",
    "print(f\"Exemple : {df['category_groups'].iloc[0]}\")\n",
    "\n",
    "# Faire la conversion\n",
    "def convert_to_list(string):\n",
    "    li = list(string.split(\",\"))\n",
    "    return li\n",
    "\n",
    "if type(df[\"category_groups\"].iloc[0]) != list:\n",
    "    df[\"category_groups\"] = df[\"category_groups\"].apply(convert_to_list)\n",
    "    print(\"\\n✓ Conversion effectuée\")\n",
    "\n",
    "# Vérifier APRÈS la conversion\n",
    "print(\"\\nAPRÈS la conversion :\")\n",
    "print(f\"Type : {type(df['category_groups'].iloc[0])}\")\n",
    "print(f\"Exemple : {df['category_groups'].iloc[0]}\")\n",
    "\n",
    "# Vérifier que toutes les valeurs sont des listes\n",
    "print(f\"\\nToutes les valeurs sont des listes : {df['category_groups'].apply(lambda x: isinstance(x, list)).all()}\")\n",
    "\n",
    "# Vérifier qu'on a toujours 47 lignes\n",
    "print(f\"Nombre de lignes après conversion : {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert category_groups to list\n",
    "\n",
    "def convert_to_list(string):\n",
    "    li = list(string.split(\",\"))\n",
    "    return li\n",
    "  \n",
    "if type(df[\"category_groups\"][df.index[0]]) != list:\n",
    "    df[\"category_groups\"] = [convert_to_list(x) for x in df[\"category_groups\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select companies in cybersecurity <a class=\"anchor\" id=\"cyber\"></a>\n",
    "\n",
    "\n",
    "We decide to select only companies that work in the cybersecurity field. The algorithm is easily extendible to any field: we only have to change the _field_words_ list word.\n",
    "\n",
    "Please note that if we want to select also some sub-sample, we have to cut the dataset at this stage (as it is done in the quick loop at the end of this notebook).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flag_cybersecurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b08c688b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_groups\n",
      "<class 'list'>    47\n",
      "Name: count, dtype: int64\n",
      "[['Health Care', 'Hospital', 'Medical', 'Personal Health'], ['Gaming', 'Mobile Devices', 'Video Games'], ['Analytics', 'Database'], ['Internet', 'Ticketing', 'Travel'], ['Biotechnology', 'Genetics', 'Health Diagnostics', 'Life Science'], ['Leisure'], ['Higher Education', 'Nursing and Residential Care', 'Universities'], ['Developer APIs', 'Developer Tools', 'E-Commerce', 'Mobile', 'Mobile Payments', 'Payments'], ['Digital Marketing', 'Marketing', 'Software'], ['Electronics', 'Hardware', 'Manufacturing'], ['Dental', 'Health Care', 'Manufacturing', 'Medical', 'Medical Device'], ['Universities'], ['Internet'], ['Health Care'], ['Food Processing'], ['Electronics', 'Financial Services', 'Payments', 'Sales'], ['Consulting'], ['Billing', 'Finance', 'Mobile Payments'], ['Education', 'Social Media'], ['Communities', 'Developer Tools', 'Enterprise Software', 'Location Based Services', 'Mobile', 'SaaS']]\n",
      "na: 0\n"
     ]
    }
   ],
   "source": [
    "# regarder types / exemples\n",
    "print(df['category_groups'].apply(lambda x: type(x)).value_counts())\n",
    "print(df['category_groups'].head(20).tolist())\n",
    "# compter valeurs vides / NaN\n",
    "print(\"na:\", df['category_groups'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "99dce549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avant :  (47, 18)\n",
      "après  :  (0, 18)  flag: True\n",
      "Empty DataFrame\n",
      "Columns: [uuid, name, rank, country_code, region, city, status, short_description, category_groups, category_groups_list, total_funding_usd, total_funding, founded_on, closed_on, employee_count, linkedin_url, primary_role, revenue_range]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"avant : \", df.shape)\n",
    "df, flag_cybersecurity = field_extraction('cybersecurity', df)\n",
    "print(\"après  : \", df.shape, \" flag:\", flag_cybersecurity)\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721c82c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched in category_groups: 0\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "matched in short_description: 0\n",
      "Empty DataFrame\n",
      "Columns: [uuid, name, rank, country_code, region, city, status, short_description, category_groups, category_groups_list, total_funding_usd, total_funding, founded_on, closed_on, employee_count, linkedin_url, primary_role, revenue_range]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "keywords = ['cyber', 'security', 'cybersecurity']\n",
    "mask = df['category_groups'].apply(\n",
    "    lambda lst: isinstance(lst, list) and any(k.lower() in ' '.join(lst).lower() for k in keywords)\n",
    ")\n",
    "print(\"matched in category_groups:\", mask.sum())\n",
    "print(df[mask].head(10))\n",
    "# aussi vérifier descriptions\n",
    "desc_mask = df['short_description'].astype(str).str.contains('|'.join(keywords), case=False, na=False)\n",
    "print(\"matched in short_description:\", desc_mask.sum())\n",
    "print(df[desc_mask].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>name</th>\n",
       "      <th>rank</th>\n",
       "      <th>country_code</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>status</th>\n",
       "      <th>short_description</th>\n",
       "      <th>category_groups</th>\n",
       "      <th>category_groups_list</th>\n",
       "      <th>total_funding_usd</th>\n",
       "      <th>total_funding</th>\n",
       "      <th>founded_on</th>\n",
       "      <th>closed_on</th>\n",
       "      <th>employee_count</th>\n",
       "      <th>linkedin_url</th>\n",
       "      <th>primary_role</th>\n",
       "      <th>revenue_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [uuid, name, rank, country_code, region, city, status, short_description, category_groups, category_groups_list, total_funding_usd, total_funding, founded_on, closed_on, employee_count, linkedin_url, primary_role, revenue_range]\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['short_description'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Companies and Technologies classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking\n",
    "\n",
    "I personally appreciate the ranking that you provide for each company. However, I did not quite understand what's the magic behind it. Is there any chance to get some more insight/details, also considering that we do have an NDA in place?\n",
    "\n",
    "- Crunchbase rank uses Crunchbase’s intelligent algorithms to score and rank entities (e.g. Company, People, Investors, etc.).\n",
    "- The algorithms take into account many different variables; ranging from funding events, the entity’s strength of relationships with other entities in the Crunchbase ecosystem, the level of engagement from our website, news articles, and acquisitions.\n",
    "\n",
    "    - A company’s Rank is fluid and subject to rising and decaying over time with time-sensitive events. Events such as product launches, funding events, leadership changes, and news affect a company’s Crunchbase Rank.\n",
    "\n",
    "\n",
    "- The Crunchbase rank shows where an entity falls in the Crunchbase database relative to all other entities in that entity type (i.e. if searching for companies, you will see where a specific company ranks relative to all other companies). An entity with a Crunchbase Rank of 1 has the highest rank relative to all other entities of that type.\n",
    "\n",
    "I would also suggest leveraging our Trend Score - 7 Day, 30 Day, 90 Day (e.g. Company, People, Investors, etc.)\n",
    "\n",
    "- While Rank shows context, Crunchbase Trend Score demonstrates activity. A company’s rank will change based on activity (fundraising, news, etc.) and Trend Score is an indicator of how much their rank is changing at any given time.\n",
    "- Crunchbase Trend Score tracks the fluctuations in Rank. As a company’s rank changes, so do its Trend Score.\n",
    "- Trend Score measures the rate of a company’s activity on a 20-point (+10 <-> -10) scale. Scores closer to +10 mean it’s moving up in rank much faster compared to their peers. Scores closer to -10 mean it’s moving down.\n",
    "- For example, a company that announces its first funding round will likely experience a jump in its Rank, pushing its Trend Score up as its page views, article counts, funding amount, team members, etc., begin to increase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create graph and dictionaries <a class=\"anchor\" id=\"create_graph\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the dictionaries of Companies and Technologies from the dataset and create the network\n",
    "df_limited = df  # Par défaut, utilise tout le DataFrame\n",
    "[dict_companies, dict_tech, B] = extract_classes_company_tech(df_limited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 0 companies and 0 technologies\n"
     ]
    }
   ],
   "source": [
    "print(f\"We have {len(dict_companies)} companies and {len(dict_tech)} technologies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dictionaries and network <a class=\"anchor\" id=\"save\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dictionaries in a pickle files\n",
    "\n",
    "# if flag_cybersecurity==False: # all fields\n",
    "#     name_file_com = \"savings/classes/dict_companies_\" + str(len(dict_companies)) + \".pickle\"\n",
    "#     name_file_tech = \"savings/classes/dict_tech_\" + str(len(dict_tech)) + \".pickle\"\n",
    "# else: # only companies in cybersecurity\n",
    "#     name_file_com = \"savings/classes/dict_companies_cybersecurity_\" + str(len(dict_companies)) + \".pickle\"\n",
    "#     name_file_tech = \"savings/classes/dict_tech_cybersecurity_\" + str(len(dict_tech)) + \".pickle\"\n",
    "if flag_cybersecurity==False: # all fields\n",
    "    name_file_com = \"savings/classes/dict_companies_\" + str(len(dict_companies)) + \".pickle\"\n",
    "    name_file_tech = \"savings/classes/dict_tech_\" + str(len(dict_tech)) + \".pickle\"\n",
    "else: # only companies in cybersecurity\n",
    "    name_file_com = \"savings/classes/dict_companies_cybersecurity_\" + str(len(dict_companies)) + \".pickle\"\n",
    "    name_file_tech = \"savings/classes/dict_tech_cybersecurity_\" + str(len(dict_tech)) + \".pickle\"\n",
    "\n",
    "# companies\n",
    "with open(name_file_com, \"wb\") as f:\n",
    "    pickle.dump(dict_companies, f)\n",
    "\n",
    "#technologies\n",
    "with open(name_file_tech, \"wb\") as f:\n",
    "    pickle.dump(dict_tech, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphe sauvegardé dans savings/networks/cybersecurity_comp_0_tech_0.gpickle\n"
     ]
    }
   ],
   "source": [
    "# Save the bipartite graph as gpickle:\n",
    "\n",
    "# if flag_cybersecurity==False: # all fields\n",
    "#     name_file_graph = 'savings/networks/comp_' + str(len(dict_companies)) + '_tech_' + str(len(dict_tech)) + '.gpickle'                                     \n",
    "# else: # only companies in cybersecurity\n",
    "#     name_file_graph = 'savings/networks/cybersecurity_comp_'+ str(len(dict_companies)) + '_tech_' + str(len(dict_tech)) + '.gpickle'\n",
    "                                                       \n",
    "# nx.write_gpickle(B, name_file_graph)\n",
    "\n",
    "# Save the bipartite graph as gpickle:\n",
    "\n",
    "# Save the bipartite graph as gpickle:\n",
    "if flag_cybersecurity == False:  # all fields\n",
    "    name_file_graph = 'savings/networks/comp_' + str(len(dict_companies)) + '_tech_' + str(len(dict_tech)) + '.gpickle'\n",
    "else:  # only companies in cybersecurity\n",
    "    name_file_graph = 'savings/networks/cybersecurity_comp_' + str(len(dict_companies)) + '_tech_' + str(len(dict_tech)) + '.gpickle'\n",
    "\n",
    "# Sauvegarder le graphe avec pickle\n",
    "with open(name_file_graph, \"wb\") as f:\n",
    "    pickle.dump(B, f)\n",
    "\n",
    "print(f\"Graphe sauvegardé dans {name_file_graph}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick loop  <a class=\"anchor\" id=\"loop0\"></a>\n",
    "\n",
    "With quick loop, we mean that we do all the step of the previous sections, in order to update the dictionaries, for all size, in only one loop.\n",
    "\n",
    "In this part, you won't see many comments because everything has been already explained before :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limits = [2443]\n",
    "flag_cybersecurity = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 0 companies and 0 technologies\n",
      "Graphe sauvegardé dans savings/networks/cybersecurity_comp_0_tech_0.gpickle\n"
     ]
    }
   ],
   "source": [
    "# for i in limits:\n",
    "#     df_limited = df[:i] # set limits\n",
    "#     [dict_companies, dict_tech, B] = extract_classes_company_tech(df_limited)\n",
    "#     print(f\"We have {len(dict_companies)} companies and {len(dict_tech)} technologies\")\n",
    "    \n",
    "#     # Save dictionaries in a pickle files\n",
    "\n",
    "#     if flag_cybersecurity==False: # all fields\n",
    "#         name_file_com = \"savings/classes/dict_companies_\" + str(len(dict_companies)) + \".pickle\"\n",
    "#         name_file_tech = \"savings/classes/dict_tech_\" + str(len(dict_tech)) + \".pickle\"\n",
    "#     else: # only companies in cybersecurity\n",
    "#         name_file_com = \"savings/classes/dict_companies_cybersecurity_\" + str(len(dict_companies)) + \".pickle\"\n",
    "#         name_file_tech = \"savings/classes/dict_tech_cybersecurity_\" + str(len(dict_tech)) + \".pickle\"\n",
    "\n",
    "#     # companies\n",
    "#     with open(name_file_com, \"wb\") as f:\n",
    "#         pickle.dump(dict_companies, f)\n",
    "\n",
    "#     #technologies\n",
    "#     with open(name_file_tech, \"wb\") as f:\n",
    "#         pickle.dump(dict_tech, f)\n",
    "        \n",
    "#     if flag_cybersecurity==False: # all fields\n",
    "#         name_file_graph = 'savings/networks/comp_' + str(len(dict_companies)) + '_tech_' + str(len(dict_tech)) + '.gpickle'                                     \n",
    "#     else: # only companies in cybersecurity\n",
    "#         name_file_graph = 'savings/networks/cybersecurity_comp_'+ str(len(dict_companies)) + '_tech_' + str(len(dict_tech)) + '.gpickle'\n",
    "\n",
    "#     nx.write_gpickle(B, name_file_graph)\n",
    "\n",
    "# --------------------NO LONGER WRITE_GPICKLE AVAILABLE--------------------\n",
    "\n",
    "#Alternative way to save the graph with pickle\n",
    "\n",
    "for i in limits:\n",
    "    df_limited = df[:i]  # set limits\n",
    "    [dict_companies, dict_tech, B] = extract_classes_company_tech(df_limited)\n",
    "    print(f\"We have {len(dict_companies)} companies and {len(dict_tech)} technologies\")\n",
    "    \n",
    "    # Save dictionaries in pickle files\n",
    "    if flag_cybersecurity == False:  # all fields\n",
    "        name_file_com = \"savings/classes/dict_companies_\" + str(len(dict_companies)) + \".pickle\"\n",
    "        name_file_tech = \"savings/classes/dict_tech_\" + str(len(dict_tech)) + \".pickle\"\n",
    "    else:  # only companies in cybersecurity\n",
    "        name_file_com = \"savings/classes/dict_companies_cybersecurity_\" + str(len(dict_companies)) + \".pickle\"\n",
    "        name_file_tech = \"savings/classes/dict_tech_cybersecurity_\" + str(len(dict_tech)) + \".pickle\"\n",
    "\n",
    "    # Save companies dictionary\n",
    "    with open(name_file_com, \"wb\") as f:\n",
    "        pickle.dump(dict_companies, f)\n",
    "\n",
    "    # Save technologies dictionary\n",
    "    with open(name_file_tech, \"wb\") as f:\n",
    "        pickle.dump(dict_tech, f)\n",
    "        \n",
    "    # Save the bipartite graph\n",
    "    if flag_cybersecurity == False:  # all fields\n",
    "        name_file_graph = 'savings/networks/comp_' + str(len(dict_companies)) + '_tech_' + str(len(dict_tech)) + '.gpickle'\n",
    "    else:  # only companies in cybersecurity\n",
    "        name_file_graph = 'savings/networks/cybersecurity_comp_' + str(len(dict_companies)) + '_tech_' + str(len(dict_tech)) + '.gpickle'\n",
    "\n",
    "    # Save the graph using pickle\n",
    "    with open(name_file_graph, \"wb\") as f:\n",
    "        pickle.dump(B, f)\n",
    "\n",
    "    print(f\"Graphe sauvegardé dans {name_file_graph}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
