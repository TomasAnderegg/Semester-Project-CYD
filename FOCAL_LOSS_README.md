# Guide d'utilisation de Focal Loss

## üìö Qu'est-ce que Focal Loss?

Focal Loss est une fonction de perte sp√©cialement con√ßue pour g√©rer les **d√©s√©quilibres de classes extr√™mes** (Lin et al., 2017).

Dans votre cas: **52 vrais liens sur 170,742 paires (0.03%)**

### Formule

```
FL(p) = -Œ±(1-p)^Œ≥ * log(p)    pour classe positive
FL(p) = -Œ± * p^Œ≥ * log(1-p)   pour classe n√©gative
```

**Param√®tres:**
- **gamma (Œ≥)**: Facteur de focalisation (d√©faut: 2.0)
  - Œ≥=0 ‚Üí √©quivalent √† BCE classique
  - Œ≥=2 ‚Üí r√©duit fortement l'importance des exemples faciles
  - Œ≥=5 ‚Üí tr√®s agressif

- **alpha (Œ±)**: Poids pour la classe positive (d√©faut: 0.25)
  - Œ±=0.25 ‚Üí classe positive a un poids de 0.25
  - Œ±=0.5 ‚Üí poids √©gal entre positifs et n√©gatifs

## üöÄ Comment utiliser

### Option 1: Entra√Ænement avec Focal Loss (RECOMMAND√â)

```bash
python train_self_supervised.py \
  --data crunchbase \
  --use_memory \
  --use_focal_loss \
  --focal_alpha 0.25 \
  --focal_gamma 2.0 \
  --prefix tgn-focal \
  --n_epoch 50
```

### Option 2: Entra√Ænement avec BCE classique (ANCIEN)

```bash
python train_self_supervised.py \
  --data crunchbase \
  --use_memory \
  --prefix tgn-bce \
  --n_epoch 50
```

## ‚öôÔ∏è Hyperparam√®tres recommand√©s

### Configuration par d√©faut (bonne pour commencer)
```bash
--use_focal_loss --focal_alpha 0.25 --focal_gamma 2.0
```

### Pour donn√©es tr√®s d√©s√©quilibr√©es (votre cas)
```bash
--use_focal_loss --focal_alpha 0.1 --focal_gamma 2.0
```
‚Ü≥ Donne encore plus d'importance aux rares exemples positifs

### Pour focalisation plus agressive
```bash
--use_focal_loss --focal_alpha 0.25 --focal_gamma 5.0
```
‚Ü≥ Ignore encore plus les exemples faciles

## üìä R√©sultats attendus

### Avec BCE (actuel):
- Probabilit√©s: m√©diane ~0.04, max ~0.70
- Recall@1000: 7.7% des vrais liens
- Mod√®le tr√®s conservateur

### Avec Focal Loss (attendu):
- Probabilit√©s mieux calibr√©es
- Recall@1000: **15-25%** des vrais liens (am√©lioration 2-3x)
- Meilleur ranking des vrais liens
- Top pr√©dictions plus pertinentes pour TechRank

## üß™ Test de la Focal Loss

Pour v√©rifier que Focal Loss fonctionne correctement:

```bash
python focal_loss.py
```

Ce script de test affiche:
- Comparaison BCE vs Focal Loss
- Impact du param√®tre gamma
- Comportement sur exemples faciles vs difficiles

## üìÅ Fichiers modifi√©s

1. **focal_loss.py** (NOUVEAU)
   - Impl√©mentation de FocalLoss
   - Classe FocalLoss avec param√®tres alpha et gamma
   - Tests unitaires

2. **train_self_supervised.py** (MODIFI√â)
   - Ligne 20: Import de FocalLoss
   - Lignes 84-89: Nouveaux arguments --use_focal_loss, --focal_alpha, --focal_gamma
   - Lignes 211-238: Configuration conditionnelle BCE vs Focal Loss
   - **L'ancienne BCE est COMMENT√âE, pas supprim√©e**

## üîÑ Comment revenir en arri√®re?

Si Focal Loss ne donne pas de bons r√©sultats:

1. **Ne PAS utiliser** le flag `--use_focal_loss` lors de l'entra√Ænement
2. Le code utilisera automatiquement BCE (ligne 225)
3. Aucun changement de code n√©cessaire!

```bash
# Revenir √† BCE (enlever --use_focal_loss)
python train_self_supervised.py \
  --data crunchbase \
  --use_memory \
  --prefix tgn-bce-fallback \
  --n_epoch 50
```

## üìà Comparaison des mod√®les

Pour comparer BCE vs Focal Loss, entra√Ænez deux mod√®les:

```bash
# Mod√®le 1: BCE classique
python train_self_supervised.py \
  --data crunchbase \
  --use_memory \
  --prefix tgn-bce \
  --n_epoch 50

# Mod√®le 2: Focal Loss
python train_self_supervised.py \
  --data crunchbase \
  --use_memory \
  --use_focal_loss \
  --focal_alpha 0.25 \
  --focal_gamma 2.0 \
  --prefix tgn-focal \
  --n_epoch 50
```

Puis √©valuez avec validation temporelle:

```bash
# √âvaluer mod√®le BCE
python temporal_validation_diagnostic.py \
  --data crunchbase \
  --model_path saved_models/tgn-bce-crunchbase.pth \
  --use_memory \
  --auto_detect_params

# √âvaluer mod√®le Focal Loss
python temporal_validation_diagnostic.py \
  --data crunchbase \
  --model_path saved_models/tgn-focal-crunchbase.pth \
  --use_memory \
  --auto_detect_params
```

Comparez les r√©sultats:
- Precision@K
- Recall@K
- Rang moyen des vrais liens
- Distribution des probabilit√©s

## üéØ M√©triques √† surveiller

Avec Focal Loss, vous devriez voir:

1. **Probabilit√©s des vrais liens plus √©lev√©es**
   - M√©diane devrait passer de 0.25 √† 0.40+

2. **Meilleur ranking**
   - Rang m√©dian devrait descendre de 6,609 √† <5,000

3. **Meilleur recall**
   - Recall@1000 devrait passer de 7.7% √† 15-20%

4. **Am√©lioration vs baseline al√©atoire**
   - Devrait passer de 13x √† 20-30x meilleur que le hasard

## ‚ö†Ô∏è Points d'attention

1. **Temps d'entra√Ænement**: Focal Loss est l√©g√®rement plus lent (~5-10%)
2. **Convergence**: Peut n√©cessiter plus d'epochs pour converger
3. **Hyperparam√®tres**: Commencez avec les valeurs par d√©faut, ajustez ensuite

## üìö R√©f√©rence

Lin, T. Y., Goyal, P., Girshick, R., He, K., & Doll√°r, P. (2017).
"Focal loss for dense object detection."
In Proceedings of the IEEE international conference on computer vision (pp. 2980-2988).
