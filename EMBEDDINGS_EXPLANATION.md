# Features Statiques vs Embeddings Temporels - Explication

## ğŸ” Le ProblÃ¨me avec Hard Negative Mining

Tu as demandÃ©: "je ne comprends pas quels types de embedding temporels ?"

Excellente question ! Voici la diffÃ©rence cruciale.

## 1ï¸âƒ£ Features Statiques (utilisÃ©es actuellement)

### Qu'est-ce que c'est ?

Les **features statiques** sont les attributs bruts des nodes qui **ne changent JAMAIS** pendant le training:

```python
# ChargÃ©es au dÃ©but du script (train_self_supervised.py, ligne 136)
node_features, edge_features, full_data, train_data, ... = get_data(DATA)

# node_features est un numpy array de shape (num_nodes, feature_dim)
# Par exemple: (1016, 172) = 1016 nodes, 172 features chacun
```

### Exemple concret pour un investisseur:

```
Investor ID 463:
  Features statiques = [
    0.5,    # Type: VC
    0.8,    # Focus: Tech
    0.3,    # Stage prÃ©fÃ©rÃ©: Seed
    1.2,    # Montant moyen investi (log-scale)
    0.1,    # Secteur: SantÃ©
    0.9,    # Secteur: AI
    ...     # 172 features au total
  ]
```

**Ces valeurs sont FIXES** - elles ne changent pas au cours du temps.

### Utilisation actuelle dans Hard Negative Mining:

```python
# train_self_supervised.py, lignes 315-322
node_features_np = node_features  # âš ï¸ Features STATIQUES
negatives_batch = hard_neg_sampler.sample(
    sources=sources_batch,
    destinations=destinations_batch,
    embeddings=node_features_np,  # âš ï¸ Pas adaptÃ© au contexte temporel!
    adjacency_dict=train_adjacency_dict,
    n_negatives=1
)
```

### ProblÃ¨me:

Ces features **ignorent complÃ¨tement**:
- âœ— L'historique d'investissement de l'investisseur
- âœ— Les connexions rÃ©centes dans le graphe
- âœ— L'Ã©volution temporelle des prÃ©fÃ©rences
- âœ— Le contexte du moment (tendances du marchÃ©)

**RÃ©sultat**: Le sampler peut sÃ©lectionner des "hard negatives" qui sont similaires sur papier mais ne sont pas vraiment difficiles dans le contexte temporel.

---

## 2ï¸âƒ£ Embeddings Temporels (ce qui serait mieux)

### Qu'est-ce que c'est ?

Les **embeddings temporels** sont calculÃ©s **dynamiquement** par le modÃ¨le TGN en fonction:
- De l'historique des interactions jusqu'Ã  un timestamp donnÃ©
- De la structure du graphe au moment T
- De la mÃ©moire du node (si `--use_memory` est activÃ©)

### Comment ils sont calculÃ©s:

```python
# model/tgn.py, ligne 101-156
def compute_temporal_embeddings(self, source_nodes, destination_nodes,
                                negative_nodes, edge_times, edge_idxs, n_neighbors=20):
    """
    Compute temporal embeddings for sources, destinations, and negatives.

    Ces embeddings CHANGENT Ã  chaque timestamp!
    """

    # 1. RÃ©cupÃ©rer la mÃ©moire actuelle (Ã©tat des nodes)
    if self.use_memory:
        memory = self.get_updated_memory(...)

    # 2. AgrÃ©ger les voisins temporels (interactions rÃ©centes)
    node_embedding = self.embedding_module.compute_embedding(
        memory=memory,
        source_nodes=nodes,
        timestamps=timestamps,
        n_layers=self.n_layers,
        n_neighbors=n_neighbors
    )

    return source_embedding, destination_embedding, negative_embedding
```

### Exemple concret:

**MÃªme investisseur (ID 463) Ã  diffÃ©rents moments:**

```
Timestamp 1 (2023-01-01):
  Embedding temporel = [0.2, -0.5, 0.8, ..., 0.3]  # 100 dimensions
  (Investisseur vient d'investir dans 3 startups AI)

Timestamp 2 (2023-06-01):
  Embedding temporel = [0.7, 0.1, -0.2, ..., 0.9]  # 100 dimensions
  (Investisseur a Ã©tÃ© actif, nouveau pattern dÃ©tectÃ©)

Timestamp 3 (2024-01-01):
  Embedding temporel = [-0.1, 0.3, 0.5, ..., -0.4]  # 100 dimensions
  (Investisseur moins actif, prÃ©fÃ©rences ont Ã©voluÃ©)
```

**Ces valeurs CHANGENT** en fonction du contexte temporel!

### SchÃ©ma de calcul:

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Node Features  â”‚
                    â”‚   (statiques)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Historical  â”‚â”€â”€â”€â–¶â”‚  TGN Embedding  â”‚â—€â”€â”€â”€â”‚   Memory     â”‚
â”‚ Interactions â”‚    â”‚     Module      â”‚    â”‚  (Ã©tat)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Temporal      â”‚
                    â”‚   Embedding     â”‚
                    â”‚ (contextualisÃ©) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš–ï¸ Comparaison: Features Statiques vs Embeddings Temporels

| Aspect | Features Statiques | Embeddings Temporels |
|--------|-------------------|---------------------|
| **Source** | Fichiers de donnÃ©es bruts | CalculÃ©s par le modÃ¨le TGN |
| **Taille** | (1016, 172) fixe | (1016, 100) ou (1016, 200) |
| **Ã‰volution** | âŒ Jamais | âœ… Ã€ chaque timestamp |
| **Contexte temporel** | âŒ Non | âœ… Oui (historique inclus) |
| **MÃ©moire TGN** | âŒ Non utilisÃ©e | âœ… IntÃ©grÃ©e |
| **Voisinage graphe** | âŒ Non utilisÃ© | âœ… AgrÃ©gÃ© |
| **CoÃ»t calcul** | âœ… Gratuit (dÃ©jÃ  chargÃ©) | âŒ TrÃ¨s coÃ»teux |

---

## ğŸ”´ Pourquoi Hard Negative Mining Ã©choue avec Features Statiques

### Exemple concret:

**Situation**: Nous voulons trouver des hard negatives pour l'edge:
```
Company 101 (Startup AI santÃ©) â†’ Investor 463 (VC tech, actif en AI)
Timestamp: 2024-06-20
```

### Avec Features Statiques:

Le sampler calcule la similaritÃ© basÃ©e sur les features brutes:

```python
# hard_negative_mining.py, ligne 73
similarities = np.dot(neg_embs, pos_emb)  # SimilaritÃ© des features statiques

# Top "hard negatives" sÃ©lectionnÃ©s:
# 1. Investor 471: Features similaires (VC tech, focus AI)
# 2. Investor 498: Features similaires (VC tech, secteur santÃ©)
# 3. Investor 532: Features similaires (VC, early stage)
```

**ProblÃ¨me**: Ces investisseurs ont des **features similaires** mais:
- Investor 471: N'a **jamais investi** dans la santÃ© (seulement fintech)
- Investor 498: N'investit **plus** depuis 2 ans (inactif)
- Investor 532: A dÃ©jÃ  investi dans Company 101 au timestamp 2023-12-01 (donc devrait Ãªtre positif!)

**Pourquoi?** Les features statiques ne capturent pas:
- âœ— L'historique rÃ©el d'investissement
- âœ— L'activitÃ© rÃ©cente
- âœ— Les connexions existantes dans le graphe

### Avec Embeddings Temporels (thÃ©orique):

```python
# Calculer embeddings temporels au timestamp 2024-06-20
temporal_embeddings = tgn.compute_all_node_embeddings(timestamp=2024-06-20)

# Ces embeddings capturent:
# - Investor 471: Embedding reflÃ¨te qu'il n'investit QUE dans fintech
# - Investor 498: Embedding montre l'inactivitÃ© rÃ©cente
# - Investor 532: Embedding indique connexion existante avec Company 101

# Top "hard negatives" sÃ©lectionnÃ©s (basÃ©s sur embeddings temporels):
# 1. Investor 555: Profil AI santÃ©, actif, similaire Ã  463 mais n'a PAS investi dans Company 101
# 2. Investor 602: MÃªme secteur, mÃªme stage, co-investit souvent avec 463 mais pas ici
# 3. Investor 644: Pattern d'investissement trÃ¨s proche de 463

# âœ… Ces nÃ©gatifs sont VRAIMENT difficiles car le modÃ¨le devra apprendre
# des distinctions subtiles basÃ©es sur le contexte temporel complet
```

---

## ğŸ’¡ Pourquoi on n'utilise PAS les Embeddings Temporels?

### CoÃ»t computationnel prohibitif:

```python
# Ã€ CHAQUE batch d'entraÃ®nement, il faudrait:

# 1. Calculer embeddings pour TOUS les nodes (1016 nodes)
for batch_idx in range(num_batches):
    # Ã‡a, c'est dÃ©jÃ  fait pour le batch actuel
    source_emb, dest_emb, neg_emb = tgn.compute_temporal_embeddings(
        sources_batch, destinations_batch, negatives_batch, ...
    )

    # âŒ Mais pour hard negative mining, il faudrait AUSSI:
    all_node_embeddings = tgn.compute_temporal_embeddings(
        nodes=list(range(1016)),  # TOUS les nodes!
        timestamps=[current_ts] * 1016,
        ...
    )  # âš ï¸ TRÃˆS COÃ›TEUX!

    # 2. Ensuite seulement, faire le hard sampling
    hard_negatives = hard_sampler.sample(
        embeddings=all_node_embeddings.cpu().numpy()
    )

# RÃ©sultat: Training 10-50x plus lent!
```

**Estimation du coÃ»t**:
- Sans embeddings temporels: ~2 min/epoch
- Avec embeddings temporels: ~20-100 min/epoch (10-50x plus lent!)

---

## ğŸ¯ Solutions Alternatives

### Option 1: Rester sur Focal Loss seul âœ… (RECOMMANDÃ‰)

```bash
# Pas de hard negatives, juste Focal Loss
python train_self_supervised.py \
  --use_focal_loss \
  --focal_alpha 0.25 \
  --focal_gamma 2.0
```

**Avantages**:
- âœ… Marche bien (9.85x vs random)
- âœ… Rapide (pas de surcoÃ»t)
- âœ… Simple

### Option 2: Hard Negatives basÃ©s sur le graphe ğŸ”¬

Au lieu d'utiliser des features, utiliser la **structure du graphe**:

```python
# Exemple: Ã©chantillonner des nodes Ã  distance 2-3
def graph_based_hard_negatives(src, adjacency):
    """
    Hard negatives = "amis d'amis" (distance 2 dans le graphe)
    """
    # Distance 1: voisins directs (positifs)
    neighbors_1hop = adjacency[src]

    # Distance 2: amis d'amis (hard negatives potentiels!)
    neighbors_2hop = set()
    for neighbor in neighbors_1hop:
        neighbors_2hop.update(adjacency[neighbor])

    # Retirer les voisins directs
    hard_negatives = neighbors_2hop - neighbors_1hop

    return random.sample(hard_negatives, k)
```

**Avantages**:
- âœ… Capture la structure du graphe
- âœ… Pas besoin de features ou embeddings
- âœ… Rapide Ã  calculer

**InconvÃ©nients**:
- âš ï¸ Ignore les attributs des nodes
- âš ï¸ Peut manquer de diversitÃ© si le graphe est clairsemÃ©

### Option 3: Embeddings "lÃ©gers" ğŸ’¡

Utiliser une approximation rapide:

```python
# Au lieu de recalculer embeddings complets, utiliser:
# 1. Features statiques
# 2. + DegrÃ© du node (nombre de connexions)
# 3. + ActivitÃ© rÃ©cente (nombre d'interactions dans les N derniers jours)

def enriched_features(node_id, static_features, adjacency, timestamps, current_ts):
    """Features enrichies avec contexte temporel lÃ©ger"""

    # Features statiques de base
    features = static_features[node_id].copy()

    # Ajouter degrÃ©
    degree = len(adjacency[node_id])

    # Ajouter activitÃ© rÃ©cente (30 derniers jours)
    recent_activity = sum(
        1 for ts in timestamps[node_id]
        if current_ts - ts < 30 * 24 * 3600
    )

    return np.concatenate([features, [degree, recent_activity]])
```

---

## ğŸ“‹ RÃ©sumÃ©

**Question**: Quels types d'embedding temporels?

**RÃ©ponse**:
1. **Features statiques** (actuelles): Attributs fixes des nodes, ne changent jamais
2. **Embeddings temporels** (idÃ©aux): ReprÃ©sentations calculÃ©es par TGN qui Ã©voluent avec le temps

**Pourquoi Hard Negatives Ã©choue**:
- âŒ Utilise features statiques qui ne capturent pas le contexte temporel
- âŒ "SimilaritÃ©" des features â‰  "difficultÃ©" pour le modÃ¨le

**Pourquoi on n'utilise pas embeddings temporels**:
- âŒ Trop coÃ»teux computationnellement (10-50x plus lent)
- âŒ Faudrait recalculer embeddings pour TOUS les nodes Ã  chaque batch

**Recommandation**:
- âœ… **Focal Loss seul** pour l'instant (marche bien, simple, rapide)
- ğŸ”¬ Tester **graph-based hard negatives** si tu veux vraiment amÃ©liorer
- â³ Embeddings temporels seulement si tu as beaucoup de compute et temps
