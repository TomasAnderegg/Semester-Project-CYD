"""
Script de traitement des donn√©es CrunchBase pour TechRank
Auteur: Tomas
Date: 2025

Ce script charge, nettoie et traite les donn√©es d'organisations CrunchBase,
avec option de filtrage pour les entreprises de cybers√©curit√©.
Supporte CSV et DuckDB comme sources de donn√©es.
"""

import pandas as pd
import pickle
import os
from pathlib import Path
import duckdb
import networkx as nx
import random, string, urllib, requests
from typing import List
# ============================================================================
# CONFIGURATION
# ============================================================================

# Choix de la source de donn√©es
USE_DUCKDB = True  # True = utiliser DuckDB, False = utiliser CSV

# Chemins des fichiers
DATA_PATH_DUCKDB = r"C:\Users\tjga9\Documents\Tomas\EPFL\MA3\CYD PDS\Crunchbase dataset\crunchbase.duckdb"
DATA_PATH_CSV = r"C:\Users\tjga9\Documents\Tomas\EPFL\MA3\CYD PDS\Code\TechRank\5-TechRank-main\5-TechRank-main\data\sample CB data\organizations.csv"
ENTITY_NAME_1 = "organizations"  # Nom de l'entit√© qu'on consid√®re dans DuckDB peut etre faire une structure iterative pour aller dans organization, tech, investiseement pour ne pas le faire a la main !!
ENTITY_NAME_2 = "investments"

SAVE_DIR_CLASSES = "savings/classes"
SAVE_DIR_NETWORKS = "savings/networks"

# Param√®tres de filtrage
FLAG_CYBERSECURITY = True  # True = uniquement cybers√©curit√©, False = tous les domaines
LIMITS = [10000]#[2443]  # Nombre de lignes √† traiter
CYBERSECURITY_KEYWORDS = ['quantum computing'] #Permet de selectionner la cat√©gorie de cybers√©curit√©

# ============================================================================
# FONCTIONS UTILITAIRES
# ============================================================================
def visualize_graph(B, max_companies=20, max_technologies=30):
    """Visualize the bipartite graph B using matplotlib."""
    print('\n' + '='*60)
    print('D√âBUT DE VISUALIZE_GRAPH')
    print('='*60)
    
    try:
        import matplotlib
        import matplotlib.pyplot as plt
        from networkx.algorithms import bipartite
        print(f"‚úì Matplotlib version: {matplotlib.__version__}")
        print(f"‚úì Backend: {matplotlib.get_backend()}")
    except ImportError as e:
        print(f"‚ùå Erreur d'import: {e}")
        return

    # V√©rifier le graphe
    print(f"\nüìä Informations sur le graphe complet:")
    print(f"   - N≈ìuds: {B.number_of_nodes()}")
    print(f"   - Ar√™tes: {B.number_of_edges()}")
    
    if B.number_of_nodes() == 0:
        print("‚ö†Ô∏è Graphe vide, impossible de visualiser")
        return

    # S√©parer les n≈ìuds en deux ensembles
    print("\nüìã S√©paration des n≈ìuds...")
    companies = {n for n, d in B.nodes(data=True) if d.get('bipartite') == 0}
    technologies = set(B) - companies
    
    print(f"   - Companies totales: {len(companies)}")
    print(f"   - Technologies totales: {len(technologies)}")
    
    # ====== LIMITATION DU NOMBRE DE N≈íUDS ======
    
    # S√©lectionner les top companies (par degr√© = nombre de connexions)
    company_degrees = [(c, B.degree(c)) for c in companies]
    company_degrees.sort(key=lambda x: x[1], reverse=True)
    top_companies = [c for c, _ in company_degrees[:max_companies]]
    
    # S√©lectionner les top technologies (par degr√©)
    tech_degrees = [(t, B.degree(t)) for t in technologies]
    tech_degrees.sort(key=lambda x: x[1], reverse=True)
    top_technologies = [t for t, _ in tech_degrees[:max_technologies]]
    
    print(f"\n‚úÇÔ∏è  R√©duction du graphe:")
    print(f"   - Companies affich√©es: {len(top_companies)}/{len(companies)}")
    print(f"   - Technologies affich√©es: {len(top_technologies)}/{len(technologies)}")
    
    # Cr√©er un sous-graphe avec seulement ces n≈ìuds
    nodes_to_keep = set(top_companies) | set(top_technologies)
    B_sub = B.subgraph(nodes_to_keep).copy()
    
    print(f"   - N≈ìuds dans le sous-graphe: {B_sub.number_of_nodes()}")
    print(f"   - Ar√™tes dans le sous-graphe: {B_sub.number_of_edges()}")
    
    if B_sub.number_of_nodes() == 0:
        print("‚ö†Ô∏è Sous-graphe vide apr√®s filtrage!")
        return

    # Positionner les n≈ìuds
    print("\nüìê Calcul des positions...")
    pos = dict()
    pos.update((n, (1, i)) for i, n in enumerate(top_companies))
    pos.update((n, (2, i)) for i, n in enumerate(top_technologies))
    print(f"   ‚úì {len(pos)} positions calcul√©es")

    # Dessiner le graphe
    print("\nüé® Cr√©ation de la figure...")
    try:
        plt.figure(figsize=(16, 12))
        print("   ‚úì Figure cr√©√©e")
        
        # Couleurs
        companies_in_sub = {n for n in top_companies if n in B_sub}
        node_colors = ['lightblue' if n in companies_in_sub else 'lightgreen' for n in B_sub.nodes()]
        print(f"   ‚úì {len(node_colors)} couleurs d√©finies")
        
        print("   Drawing graph...")
        nx.draw(
            B_sub, 
            pos=pos, 
            with_labels=True, 
            node_size=800,
            node_color=node_colors,
            font_size=8,
            font_weight='bold',
            edge_color='gray',
            alpha=0.7
        )
        print("   ‚úì Graphe dessin√©")
        
        plt.title(f"Bipartite Graph: Top {len(top_companies)} Companies and Top {len(top_technologies)} Technologies", 
                  fontsize=14, fontweight='bold')
        print("   ‚úì Titre ajout√©")
        
        # Sauvegarder
        plt.savefig('bipartite_graph_limited.png', dpi=300, bbox_inches='tight')
        print("   ‚úì Graphe sauvegard√©: bipartite_graph_limited.png")
        
        print("\nüì∫ Affichage...")
        plt.show()
        print("   ‚úì plt.show() appel√©")
        
    except Exception as e:
        print(f"   ‚ùå Erreur pendant le dessin: {e}")
        import traceback
        traceback.print_exc()
    
    plt.close()
    print('\n' + '='*60)
    print('FIN DE VISUALIZE_GRAPH')
    print('='*60 + '\n')

def create_directories():
    """Cr√©e les r√©pertoires de sauvegarde s'ils n'existent pas."""
    Path(SAVE_DIR_CLASSES).mkdir(parents=True, exist_ok=True)
    Path(SAVE_DIR_NETWORKS).mkdir(parents=True, exist_ok=True)
    # print("‚úì R√©pertoires de sauvegarde cr√©√©s/v√©rifi√©s")


def convert_to_list(string):
    """Convertit une cha√Æne s√©par√©e par des virgules en liste."""
    # G√©rer les Series pandas
    if isinstance(string, pd.Series):
        if len(string) == 1:
            string = string.iloc[0]
        else:
            return string.apply(convert_to_list)
    
    if pd.isna(string):
        return []
    return [item.strip() for item in str(string).split(",")]


# ============================================================================
# CHARGEMENT DES DONN√âES
# ============================================================================

def explore_duckdb(filepath, ENTITY_NAME):
    """Explore la structure d'une base DuckDB."""
    # print(f"\n{'='*60}")
    # print("EXPLORATION DE LA BASE DUCKDB")
    # print(f"{'='*60}")
    
    conn = duckdb.connect(filepath, read_only=True)
    
    # Lister les tables
    tables = conn.execute("SHOW TABLES").fetchall()
    # print(f"‚úì Tables disponibles: {[t[0] for t in tables]}")
    
    # Si la table existe, afficher ses colonnes
    if any(ENTITY_NAME in t for t in tables):
        columns = conn.execute(f"DESCRIBE {ENTITY_NAME}").fetchall()
        # print(f"\n‚úì Colonnes de la table '{TABLE_NAME}':")
        for col in columns[:10]:  # Afficher les 10 premi√®res colonnes
            # print(f"  - {col[0]}: {col[1]}")
            pass
        if len(columns) > 10:
            pass
            # print(f"  ... et {len(columns) - 10} autres colonnes")
        
        # Compter les lignes
        count = conn.execute(f"SELECT COUNT(*) FROM {ENTITY_NAME}").fetchone()[0]
        # print(f"\n‚úì Nombre total de lignes: {count:,}")
    else:
        pass
        # print(f"\n‚ö† Table '{TABLE_NAME}' non trouv√©e!")
        # print(f"  Tables disponibles: {[t[0] for t in tables]}")
    
    conn.close()
    return tables


def load_data_from_duckdb(filepath, table_name):
    """Charge les donn√©es depuis une base DuckDB."""
    # print(f"\n{'='*60}")
    # print("CHARGEMENT DES DONN√âES (DUCKDB)")
    # print(f"{'='*60}")
    
    # V√©rifier que le fichier existe
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"Fichier DuckDB introuvable: {filepath}")
    
    # print(f"‚úì Connexion √†: {Path(filepath).name}")
    
    # Explorer la base
    explore_duckdb(filepath, table_name)
    
    # Charger les donn√©es
    conn = duckdb.connect(filepath, read_only=True)
    
    # Requ√™te pour charger toutes les donn√©es
    query = f"SELECT * FROM {table_name}"
    df = conn.execute(query).fetchdf()
    
    conn.close()
    
    # print(f"\n‚úì {len(df):,} lignes charg√©es depuis la table '{table_name}'")
    # print(f"‚úì {len(df.columns)} colonnes disponibles")
    # print(f"\n‚úì Aper√ßu des colonnes:")
    for i, col in enumerate(df.columns[:15]):
        # print(f"  {i+1}. {col}")
        pass
    if len(df.columns) > 15:
        pass
        # print(f"  ... et {len(df.columns) - 15} autres colonnes")
    
    return df


def load_data_from_csv(filepath):
    """Charge les donn√©es depuis un fichier CSV."""
    # print(f"\n{'='*60}")
    # print("CHARGEMENT DES DONN√âES (CSV)")
    # print(f"{'='*60}")
    
    df = pd.read_csv(filepath)
    # print(f"‚úì {len(df):,} lignes charg√©es depuis {Path(filepath).name}")
    # print(f"‚úì {len(df.columns)} colonnes disponibles")
    
    return df


def load_data(use_duckdb=True, entity_name=ENTITY_NAME_1):
    """Charge les donn√©es depuis la source configur√©e.
    Si on a choisi DuckDB, utilise cette source, sinon CSV.
    """
    if use_duckdb:
        return load_data_from_duckdb(DATA_PATH_DUCKDB, entity_name)
    else:
        return load_data_from_csv(DATA_PATH_CSV)


# ============================================================================
# NETTOYAGE DES DONN√âES
# ============================================================================

def clean_data(df):
    """Nettoie et pr√©pare les donn√©es."""
    # print(f"\n{'='*60}")
    # print("NETTOYAGE DES DONN√âES")
    # print(f"{'='*60}")
    
    # Colonnes √† supprimer
    columns_to_drop = [
        'type', 'permalink', 'cb_url', 'created_at', 'domain',
        'address', 'state_code', 'updated_at', 'legal_name', 'roles',
        'postal_code', 'homepage_url', 'num_funding_rounds',
        'total_funding_currency_code', 'phone', 'email', 'num_exits',
        'alias1', 'alias2', 'alias3', 'logo_url', 'last_funding_on',
        'twitter_url', 'facebook_url', 'linkedin_url', 'crunchbase_url',
        'overview', 'acquisitions', 'city', 'primary_role', 'region', 'founded_on',
        'ipo', 'milestones', 'news_articles', 'status', 'country_code', 'region', 'investment_type',
        'post_money_valuation_usd', 'pre_money_valuation_usd', 'closed_on'
    ]
    
    # Renommer les colonnes (adaptable selon la source)
    """ Sert a renommer les colonnes pour uniformiser les noms entre DuckDB et CSV, 
        donc si categrory_list est present on le renomme en category_groups et 
        ainsi de suite
    """
    rename_mapping = {
        'category_list': 'category_groups',
        'category_groups_list': 'category_groups'  # Au cas o√π
    }
    
    # Colonnes o√π NaN n'est pas acceptable
    required_columns = ['category_groups', 'rank', 'short_description']
    
    # Appliquer le nettoyage
    df_clean = df.copy()
    
    # print(f"‚úì Colonnes pr√©sentes avant nettoyage: {list(df_clean.columns[:10])}...")
    
    # IMPORTANT: Supprimer les colonnes dupliqu√©es
    if df_clean.columns.duplicated().any():
        duplicated_cols = df_clean.columns[df_clean.columns.duplicated()].tolist()
        # print(f"‚ö† Colonnes dupliqu√©es d√©tect√©es: {duplicated_cols}")
        # Garder seulement la premi√®re occurrence de chaque colonne
        df_clean = df_clean.loc[:, ~df_clean.columns.duplicated()]
        # print(f"‚úì Colonnes dupliqu√©es supprim√©es")
    
    # Supprimer les colonnes inutiles
    """ Supprimer seulement les colonnes qu'on a defini dans columns_to_drop et 
        qui existent dans le DataFrame
    """
    cols_to_drop = [col for col in columns_to_drop if col in df_clean.columns]
    df_clean = df_clean.drop(columns=cols_to_drop)
    # print(f"‚úì {len(cols_to_drop)} colonnes supprim√©es")
    
    # Renommer (seulement les colonnes qui existent)
    actual_renames = {k: v for k, v in rename_mapping.items() if k in df_clean.columns}
    if actual_renames:
        df_clean = df_clean.rename(columns=actual_renames)
        # print(f"‚úì Colonnes renomm√©es: {actual_renames}")
    
    # V√©rifier que les colonnes requises existent
    missing_cols = [col for col in required_columns if col not in df_clean.columns]
    if missing_cols:
        # print(f"\n‚ö† ATTENTION: Colonnes manquantes: {missing_cols}")
        # print(f"  Colonnes disponibles: {list(df_clean.columns)}")
        raise ValueError(f"Colonnes requises manquantes: {missing_cols}")
    
    # Supprimer les lignes avec NaN dans les colonnes requises
    before = len(df_clean)
    df_clean = df_clean.dropna(subset=required_columns)
    # print(f"‚úì {before - len(df_clean):,} lignes supprim√©es (valeurs manquantes)")
    
    # Trier par rang
    if 'rank' in df_clean.columns:
        df_clean = df_clean.sort_values('rank').reset_index(drop=True)
        # print(f"‚úì Donn√©es tri√©es par 'rank'")
    
    return df_clean


def process_category_groups(df):
    """Convertit la colonne category_groups en listes."""
    # print(f"\n{'='*60}")
    # print("TRAITEMENT DES CAT√âGORIES")
    # print(f"{'='*60}")
    
    df_proc = df.copy()
    
    # V√©rifier que la colonne existe
    if "category_groups" not in df_proc.columns:
        raise ValueError(f"Colonne 'category_groups' introuvable. Colonnes disponibles: {list(df_proc.columns)}")
    
    # S'assurer qu'il n'y a qu'une seule colonne category_groups
    if df_proc.columns.duplicated().any():
        # print(f"‚ö† Colonnes dupliqu√©es encore pr√©sentes, nettoyage...")
        df_proc = df_proc.loc[:, ~df_proc.columns.duplicated()]
    
    # Obtenir la Series (pas DataFrame)
    col_series = df_proc['category_groups']
    
    # V√©rifier que c'est bien une Series
    if not isinstance(col_series, pd.Series):
        # print(f"‚ö† 'category_groups' est un {type(col_series)}, conversion en Series...")
        col_series = df_proc['category_groups'].squeeze()
    
    # print(f"‚úì Type de la colonne: {type(col_series)}")
    
    # V√©rifier le type de la premi√®re valeur non-nulle
    first_valid_idx = col_series.first_valid_index()
    if first_valid_idx is not None:
        first_valid = col_series.loc[first_valid_idx]
        # print(f"‚úì Exemple de valeur: '{first_valid[:50]}...' (type: {type(first_valid)})")
        
        # Convertir en liste si ce n'est pas d√©j√† une liste
        if not isinstance(first_valid, list):
            df_proc['category_groups'] = col_series.apply(convert_to_list)
            # print(f"‚úì Conversion des cat√©gories en listes effectu√©e")
        else:
            pass
            # print(f"‚úì Les cat√©gories sont d√©j√† au format liste")
    else:
        pass
        # print(f"‚ö† Aucune valeur valide trouv√©e dans category_groups")
    
    # Statistiques
    # print(f"‚úì Valeurs NaN: {df_proc['category_groups'].isna().sum()}")
    # print(f"‚úì Exemples de cat√©gories:")
    for i in range(min(3, len(df_proc))):
        cats = df_proc['category_groups'].iloc[i]
        print(f"  {i+1}. {cats}")
    
    return df_proc


def filter_cybersecurity(df, keywords):
    """Filtre les entreprises de cybers√©curit√©."""
    print(f"\n{'='*60}")
    print("FILTRAGE CYBERS√âCURIT√â")
    print(f"{'='*60}")
    
    # Recherche dans category_groups
    mask_cat = df['category_groups'].apply(
        lambda lst: isinstance(lst, list) and 
        any(k.lower() in ' '.join(lst).lower() for k in keywords)
    )
    
    # Recherche dans short_description
    mask_desc = df['short_description'].astype(str).str.contains(
        '|'.join(keywords), case=False, na=False
    )
    
    # Combinaison des masques
    mask_combined = mask_cat | mask_desc
    
    df_filtered = df[mask_combined].reset_index(drop=True)
    
    print(f"‚úì Correspondances dans category_groups: {mask_cat.sum():,}")
    print(f"‚úì Correspondances dans short_description: {mask_desc.sum():,}")
    print(f"‚úì Total d'entreprises cybers√©curit√©: {len(df_filtered):,}")
    
    if len(df_filtered) > 0:
        pass
        # print(f"\n  Exemples d'entreprises filtr√©es:")
        for i, row in df_filtered.head(3).iterrows():
            pass
            # print(f"  - {row.get('name', 'N/A')}: {row['category_groups']}")
    
    return df_filtered


# ============================================================================
# EXTRACTION ET SAUVEGARDE
# ============================================================================
def extract_classes_company_tech(df):
    """Extracts the dictionaries of Companies and Technologies 
    from the dataset and create the network
    
    Args:
        - df: dataset

    Return:
        - dict_companies: dictionary of companies
        - dict_tech: dictionary of technologies
        - B: graph that links companies and technologies 
    """
 
    # from geopy.geocoders import Nominatim
    import classes  # tes classes Company et Technology
    print('INSIDE EXTRACT FUNCTION')
    print(f"DataFrame shape: {df.shape}")  # ‚Üê AJOUTEZ CECI
    print(f"DataFrame columns: {df.columns.tolist()}")  # ‚Üê AJOUTEZ CECI
    
    dict_companies = {}
    dict_tech = {}
    B = nx.Graph() #creation d'un graph vide no orient√©

    # Boucle sur chaque ligne du DataFrame
    for index, row in df.iterrows():
        # Cr√©ation du nom de l'entreprise
        comp_name = row['name']

        # Exemple : cr√©er l'objet Company
        c = classes.Company(
            id=row.get('uuid', index),
            name=comp_name,
            technologies=row.get('category_groups', []),

        )

        dict_companies[comp_name] = c # on sauvegarde sous le nom de l'entreprise les infos de l'entreprises (uuid, nom, categories, tot_previous_investments, num_previous_investments)
        B.add_node(comp_name, bipartite=0) #creation d'un noeud avec le nom de la comapagnie et bipartite=0 correspond a la premiere entite (dans ce cas compagnie)

        # Technologies
        categories = row.get('category_groups', [])
        if not isinstance(categories, list):
            categories = [categories]

        for tech in categories:
            if tech not in dict_tech:
                t = classes.Technology(name=tech)
                dict_tech[tech] = t
                B.add_node(tech, bipartite=1)

            # Lien entreprise ‚Üí technologie
            B.add_edge(comp_name, tech)
    print('INSIDE EXTRACT FUNCTION 2')
    print(f"Total nodes in graph: {B.number_of_nodes()}")  # ‚Üê AJOUTEZ CECI
    print(f"Total edges in graph: {B.number_of_edges()}")  # ‚Üê AJOUTEZ CECI

    return dict_companies, dict_tech, B

def extract_classes_investment(df_funding_rounds, df_invest):
 
    # from geopy.geocoders import Nominatim
    import classes  # tes classes Company et Technology
    print('INSIDE EXTRACT FUNCTION')
    print(f"DataFrame shape: {df_funding_rounds.shape}")  # ‚Üê AJOUTEZ CECI
    print(f"DataFrame columns: {df_funding_rounds.columns.tolist()}")  # ‚Üê AJOUTEZ CECI
    
    funding_round_ids = df_invest['funding_round_uuid'].tolist()
    B = nx.Graph() #creation d'un graph vide no orient√©

    # Boucle sur chaque ligne du DataFrame
    matching_rows_funding_rounds = df_funding_rounds[df_funding_rounds['uuid'].isin(funding_round_ids)]

    i = classes.Investor(
        name=matching_rows_funding_rounds['orga_name'],
        raised_money_usd=matching_rows_funding_rounds['raised_amount_usd'],
        funding_round_id=funding_round_ids

    )

    for index, row in df_funding_rounds.iterrows():
        # Cr√©ation du nom de l'entreprise
        funding_round_ids = row['funding_round_uuid']
        
        B.add_node(funding_round_ids, bipartite=2) #creation d'un noeud avec le nom de la comapagnie et bipartite=0 correspond a la premiere entite (dans ce cas compagnie)
        B.add_edge(i.raised_amount_usd, tech)

    print('INSIDE EXTRACT FUNCTION 2')
    print(f"Total nodes in graph: {B.number_of_nodes()}")  # ‚Üê AJOUTEZ CECI
    print(f"Total edges in graph: {B.number_of_edges()}")  # ‚Üê AJOUTEZ CECI

    return dict_companies, dict_tech, B



def extract_and_save(df, limit, is_cybersecurity):
    """Extrait les classes et sauvegarde les r√©sultats."""
    print(f"\n{'='*60}")
    print(f"EXTRACTION ET SAUVEGARDE (limite: {limit:,} lignes)")
    print(f"{'='*60}")
    
    # Limiter les donn√©es
    df_limited = df[:limit]
    print(df_limited.head())
    # print(f"‚úì Traitement de {len(df_limited):,} entreprises")
    print(f"Colonnes disponibles: {df_limited.columns.tolist()}")  # ‚Üê AJOUTEZ
    
    # Extraction des classes (fonction √† importer depuis votre module)
    # Note: Cette fonction doit √™tre d√©finie dans votre code
    dict_companies, dict_tech, B = extract_classes_company_tech(df_limited)
    try:
        # dict_companies, dict_tech, B = extract_classes_company_tech(df_limited)
        # visualize_graph(B)
        visualize_graph(B, max_companies=5, max_technologies=10)
    # except ImportError:
    #     print("‚ö† Fonction extract_classes_company_tech non disponible")
    #     print("  Veuillez l'importer depuis votre module")
    #     return
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de la visualisation: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"‚úì {len(dict_companies):,} entreprises extraites")
    print(f"‚úì {len(dict_tech):,} technologies extraites")
    
    # G√©n√©rer les noms de fichiers
    suffix = "cybersecurity_" if is_cybersecurity else ""
    
    file_companies = f"{SAVE_DIR_CLASSES}/dict_companies_{suffix}{len(dict_companies)}.pickle"
    file_tech = f"{SAVE_DIR_CLASSES}/dict_tech_{suffix}{len(dict_tech)}.pickle"
    file_graph = f"{SAVE_DIR_NETWORKS}/{suffix}comp_{len(dict_companies)}_tech_{len(dict_tech)}.gpickle"
    
    # Sauvegarder les dictionnaires
    with open(file_companies, "wb") as f:
        pickle.dump(dict_companies, f)
    print(f"‚úì Entreprises sauvegard√©es: {file_companies}")
    
    with open(file_tech, "wb") as f:
        pickle.dump(dict_tech, f)
    print(f"‚úì Technologies sauvegard√©es: {file_tech}")
    
    # Sauvegarder le graphe
    with open(file_graph, "wb") as f:
        pickle.dump(B, f)
    print(f"‚úì Graphe sauvegard√©: {file_graph}")


# ============================================================================
# Ex√©cution principale 
# ============================================================================

def main():
    """Fonction principale orchestrant tout le pipeline."""
    print("\n" + "="*60)
    print(" "*15 + "TECHRANK - TRAITEMENT CRUNCHBASE")
    print("="*60)
    print(f"\nSource de donn√©es: {'DuckDB' if USE_DUCKDB else 'CSV'}")
    
    # Cr√©er les r√©pertoires
    create_directories()
    
    try:
        # 1. Charger les donn√©es
        df_comp_tech = load_data(use_duckdb=USE_DUCKDB, entity_name=ENTITY_NAME_1)
        df_invest = load_data(use_duckdb=USE_DUCKDB, entity_name=ENTITY_NAME_2)

        print("=== Aper√ßu de df (brut) ===")
        print(df_comp_tech.shape)          # nombre de lignes et colonnes
        print(df_comp_tech.columns)        # noms des colonnes
        print(df_comp_tech.head(5))        # les 5 premi√®res lignes
        
        # 2. Nettoyer les donn√©es
        df_comp_tech_clean = clean_data(df_comp_tech)
        df_invest_clean = clean_data(df_invest)
        
        # 3. Traiter les cat√©gories
        df_comp_tech_proc = process_category_groups(df_comp_tech_clean)
        df_invest_proc = process_category_groups(df_invest_clean)
        
        # 4. Filtrer si n√©cessaire
        if FLAG_CYBERSECURITY:
            df_comp_tech_final = filter_cybersecurity(df_comp_tech_proc, CYBERSECURITY_KEYWORDS)
            
            if len(df_comp_tech_final) == 0:
                print("\n‚ö† ATTENTION: Aucune entreprise de cybers√©curit√© trouv√©e!")
                print("  V√©rifiez les mots-cl√©s ou les donn√©es source")
                return
        else:
            df_comp_tech_final = df_comp_tech_proc
            print(f"\n‚úì Mode tous domaines: {len(df_comp_tech_final):,} entreprises")
        
        # 5. Extraire et sauvegarder pour chaque limite
        for limit in LIMITS:
            if limit > len(df_comp_tech_final):
                print(f"\n‚ö† Limite {limit:,} > donn√©es disponibles ({len(df_comp_tech_final):,})")
                print(f"  Utilisation de {len(df_comp_tech_final):,} lignes")
                limit = len(df_comp_tech_final)
            
            extract_and_save(df_comp_tech_final, limit, FLAG_CYBERSECURITY)
        
        print(f"\n{'='*60}")
        print(" "*20 + "‚úì TRAITEMENT TERMIN√â")
        print(f"{'='*60}\n")
        
    except Exception as e:
        print(f"\n‚ùå ERREUR: {type(e).__name__}")
        print(f"   {str(e)}")
        import traceback
        traceback.print_exc()


# ============================================================================
# POINT D'ENTR√âE
# ============================================================================

if __name__ == "__main__":
    main()