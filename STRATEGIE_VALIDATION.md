# StratÃ©gie de Validation : Comment DÃ©terminer si une PrÃ©diction Est Correcte

## Vue d'Ensemble

Pour Ã©valuer si le modÃ¨le TGN prÃ©dit correctement les investissements futurs, votre systÃ¨me utilise plusieurs stratÃ©gies complÃ©mentaires :

1. **Validation Temporelle** (split temporel)
2. **Ranking-Based Evaluation** (Ã©valuation par classement)
3. **MÃ©triques de Classification** (AUC, AP)

---

## 1. Validation Temporelle : Le Concept ClÃ©

### Principe Fondamental

```
Timeline des Interactions:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TRAIN â”€â”€â”€â”€â”€â”€â”€â”€â”€][â”€ VAL â”€][â”€â”€â”€â”€â”€ TEST â”€â”€â”€â”€â”€]
t=0                        t=0.7    t=0.85           t=1.0

Interactions           Interactions   Interactions
d'entraÃ®nement         validation     test
```

**RÃ¨gle d'Or** : On ne prÃ©dit JAMAIS dans le passÃ©, seulement dans le futur.

### Split Temporel

```python
# Dans votre code (utils/data_processing.py)
val_time = list(np.quantile(timestamps, 0.70))
test_time = list(np.quantile(timestamps, 0.85))

train_data = interactions[timestamps < val_time]
val_data = interactions[val_time <= timestamps < test_time]
test_data = interactions[timestamps >= test_time]
```

**Exemple concret** :
```
Dataset Crunchbase:
  - Train: Investissements de 2000 Ã  2018 (70%)
  - Val:   Investissements de 2018 Ã  2020 (15%)
  - Test:  Investissements de 2020 Ã  2023 (15%)

Question posÃ©e au modÃ¨le:
  "En 2020, quelles startups vont recevoir des investissements ?"

RÃ©ponse attendue:
  Les vrais investissements de 2020-2023 (test set)
```

---

## 2. Comment Ã‰valuer une PrÃ©diction

### Approche A : Classification Binaire (Baseline)

**Question** : Le modÃ¨le prÃ©dit-il correctement si un lien existe ou non ?

#### Processus

```python
# Pour chaque interaction test:
for (startup, investor, timestamp) in test_data:
    # 1. PrÃ©dire probabilitÃ© du VRAI lien (positif)
    pos_prob = model.predict(startup, investor, timestamp)

    # 2. Sampler un FAUX lien (nÃ©gatif)
    negative_investor = random_sample(all_investors - {investor})
    neg_prob = model.predict(startup, negative_investor, timestamp)

    # 3. CrÃ©er labels
    pred_scores = [pos_prob, neg_prob]  # Ex: [0.75, 0.23]
    true_labels = [1, 0]                # 1 = vrai, 0 = faux

    # 4. Calculer mÃ©triques
    AP = average_precision_score(true_labels, pred_scores)
    AUC = roc_auc_score(true_labels, pred_scores)
```

#### MÃ©triques

**Average Precision (AP)** :
- Aire sous la courbe Precision-Recall
- Range: [0, 1], plus Ã©levÃ© = meilleur
- RÃ©sistant au dÃ©sÃ©quilibre de classes

**AUC-ROC** :
- Aire sous la courbe ROC
- Range: [0, 1], 0.5 = hasard, 1.0 = parfait

**Votre rÃ©sultat actuel** :
```
AP:  ~0.30-0.40
AUC: ~0.70-0.80
```

**InterprÃ©tation** :
```
AP = 0.35  â†’ Le modÃ¨le est 13x meilleur que le hasard
             (hasard = 52/170742 = 0.0003)
```

---

### Approche B : Ranking-Based (Plus RÃ©aliste)

**Question** : Parmi tous les investisseurs possibles, le modÃ¨le classe-t-il le vrai investisseur en haut ?

#### Processus (ImplÃ©mentation dans evaluation.py)

```python
# Pour chaque startup dans test_data:
for (startup, true_investor, timestamp) in test_data:
    # 1. PrÃ©dire probabilitÃ© pour le VRAI investisseur
    pos_prob = model.predict(startup, true_investor, timestamp)

    # 2. Sampler 100 FAUX investisseurs
    neg_investors = random_sample(all_investors - {true_investor}, n=100)
    neg_probs = [model.predict(startup, inv, timestamp)
                 for inv in neg_investors]

    # 3. Combiner et trier
    all_probs = [pos_prob] + neg_probs  # Ex: [0.75, 0.23, 0.19, ..., 0.01]
    all_investors = [true_investor] + neg_investors

    # 4. Calculer rang du vrai investisseur
    sorted_indices = argsort(all_probs, descending=True)
    rank_of_true = where(sorted_indices == 0)[0] + 1

    # rank_of_true = 1  â†’ parfait (top-1)
    # rank_of_true = 50 â†’ mÃ©diane
    # rank_of_true = 101 â†’ pire
```

#### MÃ©triques de Ranking

##### 1. Mean Reciprocal Rank (MRR)

```python
MRR = mean(1 / rank_of_true_investor)
```

**Exemples** :
```
Startup A: vrai investisseur classÃ© #1  â†’ MRR = 1/1  = 1.00  âœ…
Startup B: vrai investisseur classÃ© #2  â†’ MRR = 1/2  = 0.50
Startup C: vrai investisseur classÃ© #10 â†’ MRR = 1/10 = 0.10
Startup D: vrai investisseur classÃ© #50 â†’ MRR = 1/50 = 0.02

MRR global = (1.00 + 0.50 + 0.10 + 0.02) / 4 = 0.405
```

**InterprÃ©tation** :
```
MRR = 0.40  â†’ En moyenne, le vrai investisseur est dans le top 2-3
MRR = 0.10  â†’ En moyenne, le vrai investisseur est dans le top 10
MRR = 0.01  â†’ En moyenne, le vrai investisseur est dans le top 100
```

##### 2. Recall@K

```python
Recall@K = fraction des vrais investisseurs dans le top K
```

**Exemples** :
```
K = 10:
  Startup A: vrai investisseur classÃ© #1  â†’ âœ… dans top 10
  Startup B: vrai investisseur classÃ© #2  â†’ âœ… dans top 10
  Startup C: vrai investisseur classÃ© #25 â†’ âŒ pas dans top 10
  Startup D: vrai investisseur classÃ© #50 â†’ âŒ pas dans top 10

  Recall@10 = 2/4 = 0.50  (50% des vrais investisseurs dans top 10)

K = 50:
  Startup A: classÃ© #1  â†’ âœ…
  Startup B: classÃ© #2  â†’ âœ…
  Startup C: classÃ© #25 â†’ âœ…
  Startup D: classÃ© #50 â†’ âœ…

  Recall@50 = 4/4 = 1.00  (100% des vrais investisseurs dans top 50)
```

**Votre rÃ©sultat observÃ©** :
```
Recall@10:   ~0.05  (5% des vrais liens dans top 10)
Recall@50:   ~0.15  (15% des vrais liens dans top 50)
Recall@1000: ~0.077 (7.7% des vrais liens dans top 1000)
```

**InterprÃ©tation** :
```
Recall@1000 = 0.077  â†’  Pour 100 startups, le modÃ¨le place le vrai
                         investisseur dans le top 1000 pour ~8 d'entre elles

Baseline alÃ©atoire = 1000/170742 = 0.006  (0.6%)
AmÃ©lioration = 0.077 / 0.006 = 13x meilleur que le hasard âœ…
```

---

## 3. ImplÃ©mentation dans Votre Code

### Fichier : evaluation/evaluation.py

```python
def eval_edge_prediction(model, negative_edge_sampler, data, n_neighbors):
    """
    Ã‰value le modÃ¨le sur les donnÃ©es test

    Args:
        model: TGN model
        negative_edge_sampler: Sampler pour nÃ©gatifs
        data: Test data (sources, destinations, timestamps)
        n_neighbors: Nombre de voisins pour GNN

    Returns:
        AP, AUC, MRR, Recall@10, Recall@50
    """

    for batch in test_data:
        sources_batch = batch.sources      # Ex: [startup_1, startup_2, ...]
        destinations_batch = batch.destinations  # Ex: [investor_A, investor_B, ...]
        timestamps_batch = batch.timestamps

        # ============================================
        # 1. PRÃ‰DICTION DES POSITIFS (vrais liens)
        # ============================================
        pos_prob = model.compute_edge_probabilities(
            sources_batch,
            destinations_batch,  # Vrais investisseurs
            timestamps_batch
        )

        # ============================================
        # 2. PRÃ‰DICTION DES NÃ‰GATIFS (faux liens)
        # ============================================
        negative_samples = negative_edge_sampler.sample(batch_size)
        neg_prob = model.compute_edge_probabilities(
            sources_batch,
            negative_samples,  # Faux investisseurs
            timestamps_batch
        )

        # ============================================
        # 3. CLASSIFICATION BINAIRE
        # ============================================
        pred_scores = [pos_prob, neg_prob]
        true_labels = [1, 0]

        AP = average_precision_score(true_labels, pred_scores)
        AUC = roc_auc_score(true_labels, pred_scores)

        # ============================================
        # 4. RANKING METRICS
        # ============================================
        # Sample 100 nÃ©gatifs supplÃ©mentaires
        num_negatives = 100
        all_neg_probs = []
        for _ in range(num_negatives):
            neg_batch = negative_edge_sampler.sample(batch_size)
            neg_prob_i = model.compute_edge_probabilities(
                sources_batch, neg_batch, timestamps_batch
            )
            all_neg_probs.append(neg_prob_i)

        # Stack: (batch_size, num_negatives)
        all_neg_probs = stack(all_neg_probs, dim=1)

        # Compute ranks
        mrr, recall_dict = compute_ranking_metrics(pos_prob, all_neg_probs)

    return AP, AUC, MRR, Recall@10, Recall@50
```

### Fonction : compute_ranking_metrics

```python
def compute_ranking_metrics(pos_scores, neg_scores):
    """
    Calcule MRR et Recall@K

    Args:
        pos_scores: (batch_size, 1) - scores des vrais liens
        neg_scores: (batch_size, num_negatives) - scores des faux liens

    Returns:
        mrr: Mean Reciprocal Rank
        recall_dict: {'recall@10': 0.05, 'recall@50': 0.15}
    """

    # 1. Combiner positifs et nÃ©gatifs
    all_scores = concat([pos_scores, neg_scores], dim=1)
    # Shape: (batch_size, 1 + num_negatives)

    # 2. Trier par score dÃ©croissant
    rankings = argsort(all_scores, dim=1, descending=True)
    # rankings[i] contient les indices triÃ©s pour la startup i

    # 3. Trouver le rang du positif (index 0)
    positive_ranks = where(rankings == 0)[1] + 1  # +1 car rang commence Ã  1

    # Exemple:
    # all_scores[0] = [0.75, 0.23, 0.65, 0.19, ...]  (pos=0.75)
    # rankings[0]   = [0, 2, 1, 3, ...]              (0 est en 1Ã¨re position)
    # positive_ranks[0] = 1                          (rang = 1)

    # 4. Calculer MRR
    mrr = mean(1.0 / positive_ranks)

    # 5. Calculer Recall@K
    recall_at_10 = mean(positive_ranks <= 10)
    recall_at_50 = mean(positive_ranks <= 50)

    return mrr, {'recall@10': recall_at_10, 'recall@50': recall_at_50}
```

---

## 4. Temporal Validation Diagnostic

### Fichier : temporal_validation_diagnostic.py

Ce script fournit une **analyse dÃ©taillÃ©e** de la performance temporelle :

```python
def run_temporal_validation_with_diagnostics(model, test_data):
    """
    Validation temporelle avec diagnostic approfondi

    Analyse:
    1. Distribution des probabilitÃ©s pour vrais vs faux liens
    2. Distribution des rangs des vrais liens
    3. Performance par quartile de degrÃ©
    4. Performance par pÃ©riode temporelle
    """

    results = []

    for (startup, true_investor, timestamp) in test_data:
        # PrÃ©dire
        pos_prob = model.predict(startup, true_investor, timestamp)

        # Sample nÃ©gatifs
        neg_investors = sample(all_investors - {true_investor}, n=1000)
        neg_probs = [model.predict(startup, inv, timestamp)
                     for inv in neg_investors]

        # Calculer rang
        all_probs = [pos_prob] + neg_probs
        rank = get_rank(pos_prob, all_probs)

        # Stocker
        results.append({
            'startup': startup,
            'true_investor': true_investor,
            'pos_prob': pos_prob,
            'rank': rank,
            'timestamp': timestamp,
            'startup_degree': degree_dict[startup]
        })

    # Analyse
    analyze_results(results)
```

**Analyses produites** :

```
1. Distribution des probabilitÃ©s:
   ===============================
   Vrais liens (positifs):
     Min:      0.0001
     MÃ©diane:  0.04      â† TrÃ¨s faible ! ModÃ¨le incertain
     Max:      0.70

   Faux liens (nÃ©gatifs):
     MÃ©diane:  0.03      â† Presque identique aux positifs !

2. Distribution des rangs:
   =======================
   Rang mÃ©dian vrais liens: 6,609 sur 170,742
   Percentile 25%:          2,341
   Percentile 75%:          85,371

   â†’ 50% des vrais liens sont classÃ©s entre #2,341 et #85,371

3. Recall@K:
   =========
   Recall@10:     0.0%      â† Aucun vrai lien dans top 10
   Recall@100:    0.0%
   Recall@1000:   7.7%      â† Seulement 7.7% dans top 1000
   Recall@10000:  38.5%

4. Performance par degrÃ©:
   ======================
   Low-degree (1-5):      Recall@1000 = 2.3%   â† TrÃ¨s mauvais
   Medium-degree (6-20):  Recall@1000 = 8.1%
   High-degree (21+):     Recall@1000 = 15.2%  â† Meilleur

   â†’ Degree bias confirmÃ©
```

---

## 5. Exemples Concrets de Validation

### Exemple 1 : PrÃ©diction RÃ©ussie âœ…

```
Startup: "QuantumTech"
Timestamp: 2022-01-15
Vrai investisseur: "Sequoia Capital"

PrÃ©dictions du modÃ¨le (top 10):
  1. Sequoia Capital      â†’ 0.78  âœ… CORRECT (rang #1)
  2. Andreessen Horowitz  â†’ 0.75
  3. Accel Partners       â†’ 0.72
  ...

MRR contribution: 1/1 = 1.00
Recall@10: âœ… (dans top 10)
```

### Exemple 2 : PrÃ©diction Moyenne âš ï¸

```
Startup: "BioQuantum"
Timestamp: 2022-03-10
Vrai investisseur: "HealthTech Ventures"

PrÃ©dictions du modÃ¨le (top 10):
  1. Accel Partners         â†’ 0.82
  2. Sequoia Capital        â†’ 0.79
  3. Y Combinator           â†’ 0.76
  ...
  47. HealthTech Ventures   â†’ 0.35  â† VRAI (rang #47)
  ...

MRR contribution: 1/47 = 0.021
Recall@10: âŒ (pas dans top 10)
Recall@50: âœ… (dans top 50)
```

### Exemple 3 : PrÃ©diction RatÃ©e âŒ

```
Startup: "StealthMode Inc."
Timestamp: 2022-06-20
Vrai investisseur: "Anonymous Angel"

PrÃ©dictions du modÃ¨le (top 10):
  1. Sequoia Capital        â†’ 0.65
  2. Accel Partners         â†’ 0.63
  ...
  85,371. Anonymous Angel   â†’ 0.001  â† VRAI (rang #85,371)

MRR contribution: 1/85371 = 0.000012
Recall@10: âŒ
Recall@1000: âŒ
Recall@10000: âŒ

Pourquoi ratÃ©?
  - Startup trÃ¨s rÃ©cente (degrÃ© = 1)
  - Investisseur atypique
  - Peu de signal dans le graphe temporel
```

---

## 6. StratÃ©gie de DÃ©cision "Correcte" vs "Incorrecte"

### Selon le Contexte d'Utilisation

| Contexte | CritÃ¨re de SuccÃ¨s | Seuil |
|----------|-------------------|-------|
| **Recommandation Top-K** | Vrai investisseur dans top K | Recall@K > 0.5 |
| **Ranking gÃ©nÃ©ral** | Vrai investisseur bien classÃ© | MRR > 0.1 |
| **Classification binaire** | Prob(vrai) > Prob(faux) | AP > 0.5 |
| **Use case rÃ©el** | Top 100 recommandations | Recall@100 > 0.2 |

### Votre Situation Actuelle

```
MÃ©triques observÃ©es:
  AP:           0.35
  AUC:          0.75
  MRR:          ~0.02   (vrai investisseur classÃ© ~#50 en moyenne)
  Recall@1000:  0.077   (7.7% des vrais dans top 1000)

Baseline alÃ©atoire:
  Recall@1000:  0.006   (0.6%)

AmÃ©lioration vs hasard:
  13x meilleur âœ…

Mais:
  Pour Ãªtre utilisable en production:
    â†’ Target: Recall@1000 > 0.20 (20%)
    â†’ Votre score: 0.077 (7.7%)
    â†’ Gap: 2.6x Ã  amÃ©liorer
```

---

## 7. Comment Focal/HAR Loss AmÃ©liore la Validation

### Avant (BCE) - ProblÃ¨me

```
Vrais liens:
  MÃ©diane prob: 0.04  â† ModÃ¨le trÃ¨s incertain

Faux liens:
  MÃ©diane prob: 0.03  â† Presque identique !

RÃ©sultat: Difficile de distinguer vrais des faux
         â†’ Mauvais ranking
         â†’ Recall@K faible
```

### AprÃ¨s (Focal Loss) - AmÃ©lioration Attendue

```
Vrais liens:
  MÃ©diane prob: 0.25-0.40  â† ModÃ¨le plus confiant âœ…

Faux liens:
  MÃ©diane prob: 0.03       â† InchangÃ©

RÃ©sultat: Meilleure sÃ©paration
         â†’ Meilleur ranking
         â†’ Recall@K amÃ©liorÃ© (2-3x)
```

**MÃ©canisme** :
```
Focal Loss force le modÃ¨le Ã :
  1. Ignorer les faux liens faciles (prob dÃ©jÃ  faible)
  2. Se concentrer sur les vrais liens difficiles (prob trop faible)

â†’ Vrais liens ont des probs plus Ã©levÃ©es
â†’ Meilleur classement dans le ranking
â†’ Recall@K augmente
```

---

## 8. MÃ©triques de Validation : Tableau RÃ©capitulatif

| MÃ©trique | Formule | Range | Bon Score | Votre Score Actuel | Objectif |
|----------|---------|-------|-----------|-------------------|----------|
| **AP** | Aire sous PR curve | [0, 1] | > 0.5 | 0.35 | 0.50+ |
| **AUC** | Aire sous ROC curve | [0, 1] | > 0.7 | 0.75 | 0.85+ |
| **MRR** | mean(1/rank) | [0, 1] | > 0.1 | ~0.02 | 0.10+ |
| **Recall@10** | % vrais dans top 10 | [0, 1] | > 0.1 | 0.00 | 0.05+ |
| **Recall@50** | % vrais dans top 50 | [0, 1] | > 0.2 | ~0.05 | 0.15+ |
| **Recall@1000** | % vrais dans top 1000 | [0, 1] | > 0.2 | 0.077 | 0.20+ |

---

## 9. Commandes pour Valider Votre ModÃ¨le

### Validation Standard

```bash
# Ã‰valuer un modÃ¨le entraÃ®nÃ©
python temporal_validation_diagnostic.py \
  --data crunchbase \
  --model_path saved_models/tgn-focal-crunchbase.pth \
  --use_memory \
  --auto_detect_params
```

### Validation DÃ©taillÃ©e avec Analyse

Le script produit :
- Distribution des probabilitÃ©s
- Distribution des rangs
- Analyse par degrÃ©
- Fichiers CSV avec rÃ©sultats dÃ©taillÃ©s

---

## Conclusion

**StratÃ©gie rÃ©sumÃ©e** :

1. âœ… **Split temporel** : EntraÃ®ner sur passÃ©, prÃ©dire le futur
2. âœ… **Ranking-based** : Classer tous les candidats, pas juste 0/1
3. âœ… **MÃ©triques multiples** : AP, AUC, MRR, Recall@K
4. âœ… **Diagnostic approfondi** : Analyser oÃ¹ et pourquoi le modÃ¨le Ã©choue

**Pour "correcte" ou "incorrecte"** :
- **Classification** : Prob(vrai) > Prob(faux) â†’ correcte
- **Ranking** : Vrai investisseur dans top K â†’ correcte
- **En pratique** : Recall@1000 > 0.20 pour Ãªtre utile

**Avec Focal/HAR Loss, vous visez** :
```
Recall@1000: 0.077 â†’ 0.20  (amÃ©lioration 2.6x)
MRR: 0.02 â†’ 0.10           (amÃ©lioration 5x)
```

C'est ce que vous allez mesurer aprÃ¨s entraÃ®nement ! ğŸ¯
