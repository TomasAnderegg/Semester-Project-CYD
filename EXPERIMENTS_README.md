# Guide d'ExÃ©cution des ExpÃ©riences

Ce guide explique comment lancer toutes les configurations d'entraÃ®nement pour comparer les loss functions.

## ğŸ“‹ Configurations TestÃ©es

Les scripts lancent automatiquement **4 configurations** :

1. **BCE (Baseline)** : Binary Cross-Entropy standard
2. **Focal Loss** : Focus sur les exemples difficiles (Î±=0.25, Î³=2.0)
3. **DCL Loss** : Correction du biais de degrÃ© (Î±=0.5, Ï„=0.07)
4. **Hybrid** : Focal + DCL combinÃ©s

Chaque configuration est exÃ©cutÃ©e avec **6 runs** de **50 epochs** chacun.

## ğŸš€ Lancement Rapide

### Option 1 : Script Python (RecommandÃ©)

```bash
python run_all_experiments.py
```

**Avantages** :
- âœ… Multiplateforme (Windows, Linux, Mac)
- âœ… Gestion des erreurs
- âœ… RÃ©sumÃ© dÃ©taillÃ© en fin d'exÃ©cution
- âœ… Estimation du temps

### Option 2 : Script Batch (Windows)

```bash
run_all_experiments.bat
```

### Option 3 : Script Bash (Linux/Mac)

```bash
bash run_all_experiments.sh
```

### Option 4 : Lancer Manuellement

Si tu veux lancer une seule configuration :

```bash
# BCE (Baseline)
python train_self_supervised.py --use_memory --prefix tgn-bce \
    --n_epoch 50 --patience 10 --lr 1e-4 \
    --node_dim 200 --time_dim 200 --memory_dim 200 --message_dim 200 \
    --n_runs 6 --use_wandb

# Focal Loss
python train_self_supervised.py --use_memory --prefix tgn-focal \
    --n_epoch 50 --patience 10 --lr 1e-4 \
    --node_dim 200 --time_dim 200 --memory_dim 200 --message_dim 200 \
    --n_runs 6 --use_wandb \
    --use_focal_loss --focal_alpha 0.25 --focal_gamma 2.0

# DCL Loss
python train_self_supervised.py --use_memory --prefix tgn-dcl \
    --n_epoch 50 --patience 10 --lr 1e-4 \
    --node_dim 200 --time_dim 200 --memory_dim 200 --message_dim 200 \
    --n_runs 6 --use_wandb \
    --use_dcl_loss --dcl_alpha 0.5 --dcl_temperature 0.07

# Hybrid (Focal + DCL)
python train_self_supervised.py --use_memory --prefix tgn-hybrid \
    --n_epoch 50 --patience 10 --lr 1e-4 \
    --node_dim 200 --time_dim 200 --memory_dim 200 --message_dim 200 \
    --n_runs 6 --use_wandb \
    --use_focal_loss --use_dcl_loss \
    --focal_alpha 0.25 --focal_gamma 2.0 \
    --dcl_alpha 0.5 --dcl_temperature 0.07
```

## â±ï¸ DurÃ©e EstimÃ©e

- **Par run** : ~10-30 minutes (dÃ©pend du GPU et du dataset)
- **Par configuration** : ~1-3 heures (6 runs)
- **Total (4 configurations)** : **~4-12 heures**

## ğŸ“Š Visualisation des RÃ©sultats

Une fois toutes les expÃ©riences terminÃ©es, visualise les rÃ©sultats :

```bash
python plot_loss_comparison.py
```

Cela gÃ©nÃ¨re :
- `loss_comparison_plots/training_loss_comparison.png` : Courbes de training loss
- `loss_comparison_plots/test_metrics_comparison.png` : Comparaison des mÃ©triques de test
- `loss_comparison_plots/validation_metrics_over_epochs.png` : Evolution des mÃ©triques de validation
- `loss_comparison_plots/summary_table.csv` : Tableau rÃ©capitulatif

## ğŸ“ Fichiers GÃ©nÃ©rÃ©s

AprÃ¨s l'exÃ©cution, tu trouveras :

```
results/
â”œâ”€â”€ tgn-bce_0.json          # RÃ©sultats BCE run 0
â”œâ”€â”€ tgn-bce_1.json          # RÃ©sultats BCE run 1
â”œâ”€â”€ ...
â”œâ”€â”€ tgn-focal_0.json        # RÃ©sultats Focal run 0
â”œâ”€â”€ ...
â”œâ”€â”€ tgn-dcl_0.json          # RÃ©sultats DCL run 0
â”œâ”€â”€ ...
â””â”€â”€ tgn-hybrid_0.json       # RÃ©sultats Hybrid run 0

saved_models/
â”œâ”€â”€ tgn-bce-crunchbase.pth
â”œâ”€â”€ tgn-focal-crunchbase.pth
â”œâ”€â”€ tgn-dcl-crunchbase.pth
â””â”€â”€ tgn-hybrid-crunchbase.pth
```

## ğŸ”§ ParamÃ¨tres des Configurations

| Configuration | Focal Loss | DCL Loss | ParamÃ¨tres |
|---------------|------------|----------|------------|
| **BCE** | âŒ | âŒ | - |
| **Focal** | âœ… | âŒ | Î±=0.25, Î³=2.0 |
| **DCL** | âŒ | âœ… | Î±=0.5, Ï„=0.07 |
| **Hybrid** | âœ… | âœ… | Focal: Î±=0.25, Î³=2.0<br>DCL: Î±=0.5, Ï„=0.07 |

### ParamÃ¨tres Communs

- **Memory** : ActivÃ©e
- **Epochs** : 50
- **Early Stopping** : Patience = 10
- **Learning Rate** : 0.0001 (1e-4)
- **Dimensions** : node=200, time=200, memory=200, message=200
- **Runs** : 6 (pour moyenner les rÃ©sultats)
- **WandB** : ActivÃ© pour logging

## ğŸ› DÃ©pannage

### Erreur "CUDA out of memory"

RÃ©duis la batch size :
```bash
python train_self_supervised.py --bs 100 ...
```

### Les runs prennent trop de temps

RÃ©duis le nombre de runs ou d'epochs :
```bash
# 3 runs au lieu de 6
--n_runs 3

# 30 epochs au lieu de 50
--n_epoch 30
```

### WandB ne fonctionne pas

DÃ©sactive WandB :
```bash
# Retire simplement --use_wandb de la commande
```

## ğŸ“ˆ Analyse des RÃ©sultats

Pour ton rapport, concentre-toi sur :

1. **Training Loss** : Quelle loss converge le plus vite ?
2. **Test Metrics** : Quelle loss donne les meilleures performances finales ?
   - MRR (Mean Reciprocal Rank)
   - Recall@10, Recall@50
   - AP (Average Precision)
3. **Validation Curves** : Quelle loss est la plus stable ?
4. **New Nodes** : Quelle loss gÃ©nÃ©ralise le mieux aux nouveaux nÅ“uds ?

## ğŸ’¡ Conseils

- âœ… Lance les expÃ©riences **overnight** (elles peuvent prendre plusieurs heures)
- âœ… VÃ©rifie que ton **GPU est disponible** avant de lancer
- âœ… **Surveille WandB** pour voir la progression en temps rÃ©el
- âœ… Garde une **copie de sauvegarde** de `results/` avant de relancer
- âœ… Compare les **moyennes sur 6 runs** plutÃ´t qu'un seul run

## ğŸ¯ Objectif

Ã€ la fin, tu auras des **rÃ©sultats statistiquement robustes** (6 runs par config) pour comparer :
- BCE (baseline)
- Focal Loss (gestion du dÃ©sÃ©quilibre de classes)
- DCL Loss (correction du biais de degrÃ©)
- Hybrid (combinaison des deux)

Cela te permettra de **quantifier l'apport** de chaque technique pour ton rapport ! ğŸ“Š
