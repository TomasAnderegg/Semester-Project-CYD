# Guide : Hybrid Focal-HAR Loss

## Que se passe-t-il si je combine `--use_focal_loss` et `--use_har_loss` ?

### R√©ponse Courte

‚úÖ **Maintenant support√©** : Si vous utilisez les deux flags ensemble, le syst√®me utilise automatiquement la **Hybrid Focal-HAR Loss** qui combine les avantages des deux approches !

```bash
python train_self_supervised.py \
  --data crunchbase \
  --use_memory \
  --use_focal_loss \
  --use_har_loss \
  --prefix tgn-hybrid \
  --n_epoch 50
```

---

## Comment √áa Marche

### Les 4 Options Disponibles

| Commande | Loss Utilis√©e | Probl√®me R√©solu |
|----------|---------------|-----------------|
| Aucun flag | **BCE** | Baseline (aucun) |
| `--use_focal_loss` | **Focal Loss** | D√©s√©quilibre de classes |
| `--use_har_loss` | **HAR Loss** | Degree bias |
| `--use_focal_loss --use_har_loss` | **Hybrid Loss** | Les deux ! |

### Formule Hybrid Loss

```python
Hybrid = degree_weight * focal_weight * BCE_loss

o√π:
  degree_weight = degree^(-har_alpha)      # Composante HAR
  focal_weight = (1 - p_t)^focal_gamma     # Composante Focal
```

**En d'autres termes** :
- **Focal Loss** : R√©duit l'importance des exemples faciles (bien class√©s)
- **HAR Loss** : R√©duit l'importance des n≈ìuds populaires (haut degr√©)
- **Hybrid** : Applique les DEUX r√©ductions simultan√©ment !

---

## Exemple Concret

Imaginons 3 paires √† pr√©dire :

### Paire 1 : Startup Populaire + Lien Facile
```
"DeepMind" (degr√©=50) + "Google Ventures"
‚Üí Pattern √©vident, mod√®le pr√©dit p=0.95

BCE:          loss = 0.05
Focal:        loss = 0.0025      (r√©duit car facile)
HAR:          loss = 0.007       (r√©duit car degr√© √©lev√©)
HYBRID:       loss = 0.0004      ‚úÖ Doublement r√©duit !
```

### Paire 2 : Startup Populaire + Lien Difficile
```
"DeepMind" (degr√©=50) + "Niche VC"
‚Üí Pattern difficile, mod√®le pr√©dit p=0.35

BCE:          loss = 1.05
Focal:        loss = 0.44        (augment√© car difficile)
HAR:          loss = 0.15        (r√©duit car degr√© √©lev√©)
HYBRID:       loss = 0.06        ‚öñÔ∏è Balance focal ‚Üë et HAR ‚Üì
```

### Paire 3 : Startup √âmergente + Lien Difficile ‚≠ê
```
"StealthQuantum" (degr√©=2) + "Early-Stage VC"
‚Üí Pattern difficile, mod√®le pr√©dit p=0.25

BCE:          loss = 1.39
Focal:        loss = 0.78        (augment√© car difficile)
HAR:          loss = 0.99        (augment√© car degr√© faible)
HYBRID:       loss = 1.84        ‚úÖ Doublement augment√© !
                                 ‚Üí MAXIMUM FOCUS sur ce cas
```

---

## Visualisation

```
                FOCAL LOSS
                    |
         Facile     |     Difficile
                    |
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                    |
    R√©duit     HAR  |  Augment√©
               LOSS |
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                    |
         Haut       |      Bas
               Degr√©|

HYBRID LOSS combine les deux axes:

  Haut degr√©,  Facile     ‚Üí  ‚óè‚óè     (Tr√®s r√©duit)
  Haut degr√©,  Difficile  ‚Üí  ‚óè‚óè‚óè    (Mod√©r√©)
  Bas degr√©,   Facile     ‚Üí  ‚óè‚óè‚óè‚óè   (Mod√©r√©)
  Bas degr√©,   Difficile  ‚Üí  ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè  (MAXIMUM FOCUS !)
```

---

## Pourquoi C'est Utile pour Vous

### Votre Situation

```
Dataset:
  - 0.03% positifs (52 sur 170,742)      ‚Üí D√©s√©quilibre EXTR√äME
  - Probable degree bias                  ‚Üí Startups √©mergentes ignor√©es

Objectif:
  - Identifier startups prometteuses
  - SURTOUT les √©mergentes (low-degree)
```

### Comparaison des Approches

| Loss | G√®re D√©s√©quilibre | G√®re Degree Bias | Pour Vous |
|------|-------------------|------------------|-----------|
| **BCE** | ‚ùå Non | ‚ùå Non | Baseline |
| **Focal** | ‚úÖ Oui | ‚ùå Non | Bon |
| **HAR** | ‚ö†Ô∏è Partiellement | ‚úÖ Oui | Bon |
| **Hybrid** | ‚úÖ Oui | ‚úÖ Oui | ‚≠ê Optimal |

### Ce Que Hybrid Apporte

**Sans Hybrid (Focal seul) :**
```
Startup √©mergente difficile (degr√©=2, p=0.25)
‚Üí Focal Loss = 0.78
‚Üí Mod√®le apprend mod√©r√©ment
```

**Avec Hybrid :**
```
Startup √©mergente difficile (degr√©=2, p=0.25)
‚Üí Focal booste car difficile: √ó1.5
‚Üí HAR booste car low-degree: √ó7
‚Üí Hybrid Loss = 1.84
‚Üí Mod√®le apprend INTENSIVEMENT ‚úÖ
```

---

## Utilisation Pratique

### Option 1 : Hybrid Basique (Recommand√©)

```bash
python train_self_supervised.py \
  --data crunchbase \
  --use_memory \
  --use_focal_loss \
  --focal_alpha 0.25 \
  --focal_gamma 2.0 \
  --use_har_loss \
  --har_alpha 0.5 \
  --prefix tgn-hybrid \
  --n_epoch 50
```

**Configuration par d√©faut** :
- `lambda_focal = 0.5` (balance 50/50 entre Focal et HAR)
- HAR temperature : n'est PAS utilis√© dans hybrid (uniquement focal_gamma)

### Option 2 : Plus de Focus sur Degr√©

```bash
# Pour favoriser davantage les low-degree nodes
python train_self_supervised.py \
  --data crunchbase \
  --use_memory \
  --use_focal_loss --focal_alpha 0.25 --focal_gamma 2.0 \
  --use_har_loss --har_alpha 0.75 \
  --prefix tgn-hybrid-strong-har \
  --n_epoch 50
```

### Option 3 : Plus de Focus sur Hard Examples

```bash
# Pour favoriser davantage les hard examples
python train_self_supervised.py \
  --data crunchbase \
  --use_memory \
  --use_focal_loss --focal_alpha 0.1 --focal_gamma 5.0 \
  --use_har_loss --har_alpha 0.5 \
  --prefix tgn-hybrid-strong-focal \
  --n_epoch 50
```

---

## Comparaison Compl√®te : Les 4 Options

### Configuration Exp√©rimentale

Pour comparer les 4 approches :

```bash
# 1. Baseline BCE
python train_self_supervised.py \
  --data crunchbase --use_memory \
  --prefix tgn-bce --n_epoch 50

# 2. Focal Loss seul
python train_self_supervised.py \
  --data crunchbase --use_memory \
  --use_focal_loss --focal_alpha 0.25 --focal_gamma 2.0 \
  --prefix tgn-focal --n_epoch 50

# 3. HAR Loss seul
python train_self_supervised.py \
  --data crunchbase --use_memory \
  --use_har_loss --har_temperature 0.07 --har_alpha 0.5 \
  --prefix tgn-har --n_epoch 50

# 4. Hybrid Loss
python train_self_supervised.py \
  --data crunchbase --use_memory \
  --use_focal_loss --focal_alpha 0.25 --focal_gamma 2.0 \
  --use_har_loss --har_alpha 0.5 \
  --prefix tgn-hybrid --n_epoch 50
```

### R√©sultats Attendus

| M√©trique | BCE | Focal | HAR | Hybrid |
|----------|-----|-------|-----|--------|
| **Recall@1000** | 7.7% | 15-20% | 10-15% | **20-30%** |
| **M√©diane prob vrais liens** | 0.04 | 0.25-0.40 | 0.15-0.30 | **0.30-0.50** |
| **Performance low-degree** | 0.45 | 0.50 | 0.70 | **0.75** |
| **Performance high-degree** | 0.91 | 0.90 | 0.85 | **0.87** |
| **Diversit√© pr√©dictions** | Faible | Moyenne | √âlev√©e | **Tr√®s √©lev√©e** |

---

## Param√®tres de Hybrid Loss

### Param√®tres Focal

- **focal_gamma** (Œ≥) : Focusing parameter
  - 0 = √©quivalent √† BCE
  - 2 = standard (recommand√©)
  - 5 = tr√®s agressif

- **focal_alpha** (Œ±) : Class balancing
  - 0.25 = favorise classe minoritaire (recommand√© pour vous)
  - 0.5 = poids √©gal

### Param√®tres HAR

- **har_alpha** : Degree reweighting
  - 0.5 = correction mod√©r√©e (recommand√©)
  - 0.75 = correction forte
  - 1.0 = correction tr√®s forte

### Lambda (√âquilibre Focal/HAR)

**Actuellement fix√© √† 0.5** dans le code (balance 50/50).

Pour modifier, √©ditez [train_self_supervised.py:275](train_self_supervised.py:275) :

```python
criterion = HybridFocalHARLoss(
    focal_gamma=args.focal_gamma,
    focal_alpha=args.focal_alpha,
    har_alpha=args.har_alpha,
    lambda_focal=0.5,  # ‚Üê Modifiez cette valeur
    reduction='mean'
)
```

**Valeurs sugg√©r√©es :**
```
lambda_focal = 0.0  ‚Üí Pure HAR (ignore Focal)
lambda_focal = 0.3  ‚Üí 30% Focal, 70% HAR
lambda_focal = 0.5  ‚Üí Balance (recommand√©)
lambda_focal = 0.7  ‚Üí 70% Focal, 30% HAR
lambda_focal = 1.0  ‚Üí Pure Focal (ignore HAR)
```

---

## Impl√©mentation

### Fichiers Cr√©√©s

1. **[hybrid_loss.py](hybrid_loss.py)** : Impl√©mentation Hybrid Loss
   - `HybridFocalHARLoss` : Version standard
   - `AdaptiveHybridLoss` : Version avec scheduling (avanc√©)

2. **Modifications [train_self_supervised.py](train_self_supervised.py)** :
   - D√©tection automatique des deux flags (ligne 266)
   - Construction degree_tensor pour hybrid (ligne 179)
   - Appel correct avec degr√©s (ligne 404-412)

### Code Hybrid Loss (Simplifi√©)

```python
class HybridFocalHARLoss(nn.Module):
    def forward(self, pos_prob, neg_prob, src_degrees, dst_degrees,
                pos_label, neg_label):
        # 1. HAR: degree weights
        w_degree = degree^(-har_alpha)

        # 2. Focal: hardness weights
        w_focal = (1 - p_t)^focal_gamma

        # 3. BCE base
        bce = -log(p)

        # 4. Combine
        loss = w_degree * w_focal * bce

        return loss.mean()
```

---

## Troubleshooting

### Probl√®me : Hybrid Loss diverge

**Causes possibles :**
1. har_alpha trop √©lev√© (correction trop agressive)
2. focal_gamma trop √©lev√© (ignore trop d'exemples)

**Solution :**
```bash
# R√©duire les param√®tres
--focal_gamma 1.0 --har_alpha 0.25
```

### Probl√®me : Pas mieux que Focal seul

**Causes possibles :**
1. Pas de degree bias dans vos donn√©es
2. Lambda mal calibr√©

**Diagnostic :**
```python
# V√©rifier degree bias (voir LOSS_FUNCTIONS_GUIDE.md)
# Si pas de bias ‚Üí rester avec Focal seul
```

### Probl√®me : Pire que BCE

**Causes possibles :**
1. Sur-correction (param√®tres trop agressifs)
2. Besoin de plus d'epochs pour converger

**Solution :**
```bash
# Augmenter epochs
--n_epoch 75

# Ou r√©duire param√®tres
--focal_gamma 1.5 --har_alpha 0.35
```

---

## Quand Utiliser Hybrid Loss

### ‚úÖ Utilisez Hybrid Loss SI :

1. **D√©s√©quilibre extr√™me** (< 1% positifs) ‚úÖ VOUS
2. **Degree bias d√©tect√©** (corr√©lation degr√©-performance > 0.5) ‚úÖ PROBABLE
3. **Objectif : identifier low-degree nodes** (startups √©mergentes) ‚úÖ VOUS
4. **Dataset assez grand** (> 50k paires) ‚úÖ VOUS (170k)

### ‚ùå NE PAS utiliser Hybrid Loss SI :

1. Dataset √©quilibr√© (ratio ~50/50)
2. Pas de degree bias (performance uniforme par degr√©)
3. Vous ciblez surtout les high-degree nodes
4. Dataset trop petit (< 10k paires)

---

## Recommandation Finale

### Pour Votre Cas Sp√©cifique

```
Votre situation:
  ‚úÖ D√©s√©quilibre extr√™me (0.03%)
  ‚úÖ Probable degree bias
  ‚úÖ Objectif: startups √©mergentes
  ‚úÖ Dataset large (170k)

RECOMMANDATION: Tester HYBRID LOSS ‚≠ê

Strat√©gie:
  1. Baseline (BCE)          ‚Üí Pour r√©f√©rence
  2. Focal Loss              ‚Üí R√©soudre d√©s√©quilibre
  3. HAR Loss               ‚Üí R√©soudre degree bias
  4. Hybrid Loss (Focal+HAR) ‚Üí R√©soudre les deux !

  Puis comparer et choisir le meilleur
```

### Commande Recommand√©e

```bash
python train_self_supervised.py \
  --data crunchbase \
  --use_memory \
  --use_focal_loss \
  --focal_alpha 0.25 \
  --focal_gamma 2.0 \
  --use_har_loss \
  --har_alpha 0.5 \
  --prefix tgn-hybrid \
  --n_epoch 50 \
  --use_wandb
```

---

## R√©sum√©

| Question | R√©ponse |
|----------|---------|
| **Que se passe-t-il avec les deux flags ?** | Active automatiquement Hybrid Loss ‚úÖ |
| **C'est mieux que Focal seul ?** | Probablement OUI pour votre cas |
| **Co√ªt computationnel ?** | ~15% overhead (vs ~5% Focal, ~10% HAR) |
| **Complexit√© ?** | Transparente (juste ajouter flag --use_har_loss) |
| **Recommand√© pour moi ?** | OUI, √† tester absolument ! |

**Prochaine √©tape :** Lancez les 4 exp√©riences et comparez ! üöÄ
