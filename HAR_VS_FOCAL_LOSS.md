# HAR Contrastive Loss vs Focal Loss pour Mitiger le Degree Bias

## Question

> "Tu vois ce que c'est que 'Hardness Adaptive Reweighted (HAR) contrastive loss' pour mitigate degree bias ?"

## R√©ponse Courte

**Oui**, HAR contrastive loss est une approche sophistiqu√©e qui cible SP√âCIFIQUEMENT le **degree bias** dans les graphes, contrairement au Focal Loss qui cible les exemples difficiles de mani√®re g√©n√©rale.

**Diff√©rence Cl√© :**
- **Focal Loss** : R√©agit √† la probabilit√© pr√©dite (agnostique √† la structure du graphe)
- **HAR Loss** : R√©agit au degr√© des n≈ìuds ET √† la difficult√© de l'exemple

---

## Qu'est-ce que le Degree Bias ?

### Probl√®me

Dans les Graph Neural Networks (GNNs), les **n≈ìuds √† haut degr√©** sont syst√©matiquement favoris√©s :

```
N≈ìud A: degr√© = 100 connexions
  ‚Üí Beaucoup de signal pour le message passing
  ‚Üí Embeddings riches et informatifs
  ‚Üí Pr√©dictions confiantes et pr√©cises
  ‚Üí Mod√®le apprend bien ces exemples

N≈ìud B: degr√© = 2 connexions
  ‚Üí Peu de signal
  ‚Üí Embeddings pauvres
  ‚Üí Pr√©dictions incertaines
  ‚Üí Mod√®le ignore ces exemples difficiles

R√©sultat: Le mod√®le est BIAIS√â vers les n≈ìuds populaires
```

### Impact dans Votre Cas (TGN + Investissements)

```
Startup populaire (50+ investisseurs) :
  ‚úì TGN g√©n√®re de bons embeddings
  ‚úì Pr√©dictions pr√©cises
  ‚úì Facile √† apprendre

Startup √©mergente (1-2 investisseurs) :
  ‚úó TGN g√©n√®re des embeddings bruit√©s
  ‚úó Pr√©dictions al√©atoires
  ‚úó Mod√®le n'apprend pas ces cas

‚Üí Votre mod√®le va "recommander" surtout des startups d√©j√† populaires !
‚Üí Moins utile pour identifier les p√©pites √©mergentes
```

---

## HAR Contrastive Loss : Explication D√©taill√©e

### Origine

**Paper :** "Graph Contrastive Learning with Adaptive Augmentation" (2021) et variantes

**Objectif :** R√©duire le degree bias en repond√©rant adaptivement les exemples selon leur difficult√© ET leur degr√©

### Architecture

HAR combine 3 composantes :

#### 1. Contrastive Learning Framework

Utilise une formulation contrastive (comme InfoNCE) au lieu de classification binaire :

```python
# Au lieu de Binary Cross-Entropy:
loss_bce = -[y * log(p) + (1-y) * log(1-p)]

# HAR utilise contrastive loss:
# Pour un anchor positif i et ses positifs P_i et n√©gatifs N_i:
loss_contrastive = -log(
    sum_{j in P_i} exp(sim(i,j) / tau)
    / [sum_{j in P_i} exp(sim(i,j) / tau) + sum_{k in N_i} exp(sim(i,k) / tau)]
)

o√π:
  - sim(i,j) = similarit√© (dot product d'embeddings)
  - tau = temp√©rature
  - P_i = exemples positifs (vrais liens)
  - N_i = exemples n√©gatifs (faux liens)
```

#### 2. Hardness-Aware Weighting

Calcule la "difficult√©" de chaque exemple :

```python
# Pour chaque paire (i, j):
hardness(i, j) = 1 - similarity(i, j)

# Si similarity √©lev√©e ‚Üí hardness faible (facile)
# Si similarity faible ‚Üí hardness √©lev√©e (difficile)
```

#### 3. Degree-Adaptive Reweighting

**C'EST LA CL√â** : Ajuste le poids selon le degr√© des n≈ìuds :

```python
# Calcul du poids adaptatif
w(i) = (degree(i))^(-alpha)

o√π:
  - alpha = hyperparam√®tre de reweighting (0.5 √† 1.0 typiquement)
  - degree(i) = degr√© du n≈ìud i

Effet:
  Haut degr√© (100) ‚Üí w = 0.01 ‚Üí Poids R√âDUIT
  Bas degr√© (2)    ‚Üí w = 0.50 ‚Üí Poids AUGMENT√â
```

#### 4. HAR Loss Finale

```python
# Pour un batch de paires (i, j):
HAR_loss = sum_{(i,j)} w(i) * w(j) * hardness(i,j) * L_contrastive(i,j)

o√π:
  - w(i), w(j) = degree-adaptive weights
  - hardness(i,j) = difficult√© de la paire
  - L_contrastive = contrastive loss de base
```

---

## Comparaison : Focal Loss vs HAR Loss

### Table Comparative

| Aspect | **Focal Loss** | **HAR Contrastive Loss** |
|--------|---------------|--------------------------|
| **Objectif Principal** | G√©rer d√©s√©quilibre de classes | Mitiger degree bias |
| **Crit√®re de Reweighting** | Probabilit√© pr√©dite p_t | Degr√© des n≈ìuds + hardness |
| **Formulation** | Classification binaire (BCE) | Contrastive learning (InfoNCE) |
| **Awareness de Structure** | ‚ùå Non (agnostique au graphe) | ‚úÖ Oui (utilise explicitement le degr√©) |
| **Target Bias** | Easy examples (bien class√©s) | High-degree nodes (populaires) |
| **Computational Cost** | L√©ger (~5% overhead) | Mod√©r√© (~20% overhead) |
| **Impl√©mentation** | Simple (1 fonction) | Complexe (n√©cessite contrastive framework) |

### Formules C√¥te √† C√¥te

**Focal Loss :**
```python
FL(p_t) = -Œ±_t * (1 - p_t)^Œ≥ * log(p_t)

Reweighting bas√© sur: p_t (probabilit√© pr√©dite)
‚Üí R√©duit importance des exemples faciles (p_t √©lev√©)
```

**HAR Loss :**
```python
HAR = sum_{(i,j)} degree(i)^(-Œ±) * degree(j)^(-Œ±) * hardness(i,j) * L_contrastive

Reweighting bas√© sur: degree(i), degree(j), hardness
‚Üí R√©duit importance des n≈ìuds populaires (degr√© √©lev√©)
‚Üí Augmente importance des n≈ìuds rares (degr√© faible)
```

---

## Visualisation : Qui Est Favoris√© ?

### Avec BCE (Baseline)

```
         Facilit√©
           |
    Easy   |   ‚óè‚óè‚óè‚óè‚óè‚óè‚óè (High-degree, easy)
           |   ‚Üê Mod√®le optimise surtout ici
           |
           |   ‚óã‚óã‚óã (Low-degree, easy)
           |
    -------|-------  Degr√©
           |
    Hard   |   ‚óã (High-degree, hard)
           |
           |   ‚óè (Low-degree, hard)
           |   ‚Üê Mod√®le ignore ces cas !
           |
         Bas          Haut

Probl√®me: Biais vers high-degree nodes
```

### Avec Focal Loss

```
         Facilit√©
           |
    Easy   |   ‚óã‚óã‚óã‚óã‚óã‚óã‚óã (High-degree, easy)
           |   ‚Üê Focal Loss R√âDUIT leur importance
           |
           |   ‚óã‚óã‚óã (Low-degree, easy)
           |   ‚Üê Focal Loss R√âDUIT aussi
           |
    -------|-------  Degr√©
           |
    Hard   |   ‚óè‚óè‚óè (High-degree, hard)
           |   ‚Üê Focal Loss se concentre ici
           |
           |   ‚óè‚óè‚óè (Low-degree, hard)
           |   ‚Üê Focal Loss se concentre aussi
           |
         Bas          Haut

Am√©lioration: Focus sur hard examples
Limite: Pas de correction du degree bias
       (high-degree hard toujours favoris√© vs low-degree hard)
```

### Avec HAR Loss

```
         Facilit√©
           |
    Easy   |   ‚óã (High-degree, easy)
           |   ‚Üê HAR r√©duit (facile + populaire)
           |
           |   ‚óã‚óã (Low-degree, easy)
           |   ‚Üê HAR maintient (facile mais rare)
           |
    -------|-------  Degr√©
           |
    Hard   |   ‚óè‚óè (High-degree, hard)
           |   ‚Üê HAR r√©duit (difficile mais populaire)
           |
           |   ‚óè‚óè‚óè‚óè‚óè (Low-degree, hard)
           |   ‚Üê HAR AUGMENTE (difficile ET rare)
           |   ‚Üê C'EST LE FOCUS PRINCIPAL !
           |
         Bas          Haut

Am√©lioration: Focus sur low-degree hard examples
R√©sultat: Mitigation du degree bias
```

---

## Exemple Concret dans Votre Dataset

### Sc√©nario : Pr√©dire Investissements

**Cas 1 : Startup Populaire + Lien Difficile**

```
Startup: "DeepMind" (degr√© = 50 investisseurs)
Candidat: "Niche VC Fund"
  ‚Üí Vrai lien mais pattern non √©vident
  ‚Üí Mod√®le pr√©dit: p = 0.35 (difficile)

Poids avec FOCAL LOSS:
  hardness = (1 - 0.35)^2 = 0.42
  ‚Üí Poids mod√©r√©

Poids avec HAR LOSS:
  w(startup) = 50^(-0.5) = 0.14  ‚Üê P√©nalit√© degr√© √©lev√©
  w(investor) = ?
  hardness = high
  ‚Üí Poids r√©duit malgr√© difficult√©

R√©sultat: Focal Loss favorise plus que HAR
```

**Cas 2 : Startup √âmergente + Lien Difficile**

```
Startup: "StealthQuantum" (degr√© = 2 investisseurs)
Candidat: "Early-Stage VC"
  ‚Üí Vrai lien mais peu de signal
  ‚Üí Mod√®le pr√©dit: p = 0.25 (tr√®s difficile)

Poids avec FOCAL LOSS:
  hardness = (1 - 0.25)^2 = 0.56
  ‚Üí Poids √©lev√©

Poids avec HAR LOSS:
  w(startup) = 2^(-0.5) = 0.71  ‚Üê Boost degr√© faible !
  w(investor) = ?
  hardness = very high
  ‚Üí Poids TR√àS √©lev√©

R√©sultat: HAR booste encore plus que Focal Loss
```

**Cas 3 : Startup Populaire + Lien Facile**

```
Startup: "OpenAI" (degr√© = 80 investisseurs)
Candidat: "Microsoft Ventures"
  ‚Üí Pattern √©vident
  ‚Üí Mod√®le pr√©dit: p = 0.95 (facile)

Poids avec FOCAL LOSS:
  hardness = (1 - 0.95)^2 = 0.0025
  ‚Üí Poids tr√®s r√©duit

Poids avec HAR LOSS:
  w(startup) = 80^(-0.5) = 0.11  ‚Üê Double p√©nalit√© !
  w(investor) = ?
  hardness = low
  ‚Üí Poids EXTR√äMEMENT r√©duit

R√©sultat: HAR r√©duit encore plus que Focal Loss
```

---

## Avantages et Inconv√©nients

### Focal Loss

**‚úÖ Avantages :**
- Simple √† impl√©menter (d√©j√† fait dans votre code)
- Rapide (overhead ~5%)
- Efficace pour d√©s√©quilibre de classes
- Fonctionne avec votre BCE actuel
- Pas besoin de conna√Ætre la structure du graphe

**‚ùå Inconv√©nients :**
- Ne corrige PAS le degree bias
- Agnostique √† la structure du graphe
- Peut quand m√™me favoriser high-degree nodes si faciles

**Votre Situation :**
```
Dataset: 52 positifs sur 170,742 (0.03%)
‚Üí D√©s√©quilibre EXTR√äME
‚Üí Focal Loss tr√®s appropri√© ‚úì

Degree bias: Probablement pr√©sent
‚Üí Focal Loss ne le corrige pas ‚úó
```

---

### HAR Contrastive Loss

**‚úÖ Avantages :**
- Corrige SP√âCIFIQUEMENT le degree bias
- Force le mod√®le √† apprendre les low-degree nodes
- Am√©liore la diversit√© des pr√©dictions
- Meilleur pour d√©couvrir des "p√©pites" √©mergentes

**‚ùå Inconv√©nients :**
- Complexe √† impl√©menter (n√©cessite refonte majeure)
- Co√ªt computationnel plus √©lev√© (~20% overhead)
- N√©cessite framework contrastive (different de BCE)
- Plus d'hyperparam√®tres √† tuner (Œ±, temp√©rature)
- Pas de garantie d'am√©lioration si degree bias faible

**Votre Situation :**
```
Objectif: Identifier startups prometteuses
‚Üí Beaucoup sont probablement low-degree (√©mergentes)
‚Üí HAR serait b√©n√©fique ‚úì

Mais:
‚Üí Impl√©mentation complexe
‚Üí Focal Loss d√©j√† impl√©ment√© et pas encore test√©
‚Üí Mieux vaut d'abord √©valuer Focal Loss
```

---

## Diagnostic : Avez-vous un Degree Bias ?

### Comment D√©tecter

Apr√®s entra√Ænement avec votre mod√®le actuel, analysez :

```python
# 1. Corr√©lation degr√© vs performance
import pandas as pd
import numpy as np

# Charger vos r√©sultats de pr√©diction
df = pd.read_csv('predictions.csv')

# Calculer le degr√© de chaque startup
startup_degrees = graph.degree()  # Votre graphe bipartite

# Analyser corr√©lation
df['degree'] = df['startup_id'].map(startup_degrees)

# Grouper par quartiles de degr√©
df['degree_quartile'] = pd.qcut(df['degree'], q=4, labels=['Q1-Low', 'Q2', 'Q3', 'Q4-High'])

# Comparer performance par quartile
performance = df.groupby('degree_quartile').agg({
    'probability': 'mean',  # Probabilit√© moyenne pr√©dite
    'is_correct': 'mean'    # Accuracy
})

print(performance)
```

**Si degree bias pr√©sent :**
```
degree_quartile  probability  is_correct
Q1-Low           0.15         0.45        ‚Üê Mauvaise performance
Q2               0.35         0.62
Q3               0.58         0.78
Q4-High          0.82         0.91        ‚Üê Excellente performance

‚Üí Forte corr√©lation degr√©-performance
‚Üí HAR Loss serait b√©n√©fique
```

**Si pas de degree bias :**
```
degree_quartile  probability  is_correct
Q1-Low           0.68         0.83
Q2               0.71         0.85
Q3               0.69         0.84
Q4-High          0.72         0.86

‚Üí Performance uniforme
‚Üí HAR Loss pas n√©cessaire
```

---

## Recommandation pour Votre Cas

### Strat√©gie Progressive

**Phase 1 : Utiliser Focal Loss d'abord** ‚úÖ (Vous √™tes ici)

```bash
# D√©j√† impl√©ment√© !
python train_self_supervised.py \
  --data crunchbase \
  --use_memory \
  --use_focal_loss \
  --focal_alpha 0.25 \
  --focal_gamma 2.0 \
  --prefix tgn-focal \
  --n_epoch 50
```

**Pourquoi ?**
- D√©j√† impl√©ment√©
- R√©sout votre probl√®me principal (d√©s√©quilibre extr√™me 0.03%)
- Quick win
- Permet d'√©tablir une baseline

**Phase 2 : Diagnostic Degree Bias**

```python
# Apr√®s entra√Ænement avec Focal Loss:
python analyze_degree_bias.py \
  --model_path saved_models/tgn-focal-crunchbase.pth \
  --output_dir degree_analysis/
```

**Phase 3 : D√©cision HAR**

**Si degree bias d√©tect√© (corr√©lation > 0.5) :**
‚Üí Impl√©menter HAR Loss vaut le coup

**Si degree bias faible (corr√©lation < 0.3) :**
‚Üí Rester avec Focal Loss (suffisant)

---

## Impl√©mentation de HAR (Si N√©cessaire)

### Architecture N√©cessaire

```python
class HARContrastiveLoss(nn.Module):
    """
    Hardness Adaptive Reweighted Contrastive Loss
    pour mitiger le degree bias dans les GNNs.
    """

    def __init__(self, temperature=0.07, alpha=0.5):
        """
        Args:
            temperature: Temp√©rature pour contrastive loss
            alpha: Exposant pour degree reweighting (0.5-1.0)
        """
        super().__init__()
        self.temperature = temperature
        self.alpha = alpha

    def compute_degree_weights(self, node_ids, degree_dict):
        """
        Calcule les poids bas√©s sur le degr√© des n≈ìuds.

        w(i) = degree(i)^(-alpha)
        """
        degrees = torch.tensor([degree_dict[nid] for nid in node_ids])
        weights = torch.pow(degrees, -self.alpha)
        return weights

    def compute_hardness(self, embeddings_i, embeddings_j):
        """
        Calcule la difficult√© de chaque paire.

        hardness = 1 - similarity
        """
        similarity = F.cosine_similarity(embeddings_i, embeddings_j, dim=-1)
        hardness = 1 - similarity
        return hardness

    def forward(self, embeddings_anchor, embeddings_positive, embeddings_negative,
                anchor_ids, positive_ids, negative_ids, degree_dict):
        """
        Args:
            embeddings_anchor: Embeddings des n≈ìuds anchor (N, D)
            embeddings_positive: Embeddings des positifs (N, K, D)
            embeddings_negative: Embeddings des n√©gatifs (N, M, D)
            anchor_ids: IDs des anchors
            positive_ids: IDs des positifs
            negative_ids: IDs des n√©gatifs
            degree_dict: Dictionnaire {node_id: degree}

        Returns:
            HAR contrastive loss
        """
        batch_size = embeddings_anchor.size(0)

        # 1. Degree-adaptive weights
        w_anchor = self.compute_degree_weights(anchor_ids, degree_dict)
        # w_positive et w_negative similaires

        # 2. Compute similarities
        # Positifs
        sim_pos = F.cosine_similarity(
            embeddings_anchor.unsqueeze(1),
            embeddings_positive,
            dim=-1
        ) / self.temperature  # Shape: (N, K)

        # N√©gatifs
        sim_neg = F.cosine_similarity(
            embeddings_anchor.unsqueeze(1),
            embeddings_negative,
            dim=-1
        ) / self.temperature  # Shape: (N, M)

        # 3. Hardness for positives
        hardness_pos = self.compute_hardness(
            embeddings_anchor.unsqueeze(1),
            embeddings_positive
        )  # Shape: (N, K)

        # 4. Contrastive loss with reweighting
        logits = torch.cat([sim_pos, sim_neg], dim=1)  # (N, K+M)
        labels = torch.zeros(batch_size, dtype=torch.long).to(logits.device)  # Positifs en premier

        # Standard InfoNCE
        loss_base = F.cross_entropy(logits, labels, reduction='none')  # (N,)

        # Reweight by degree and hardness
        w_total = w_anchor * hardness_pos.mean(dim=1)  # (N,)
        loss_weighted = (loss_base * w_total).mean()

        return loss_weighted
```

### Int√©gration dans TGN

```python
# Dans train_self_supervised.py

if args.use_har_loss:
    # Construire dictionnaire de degr√©s
    degree_dict = build_degree_dict(full_data)

    criterion = HARContrastiveLoss(
        temperature=args.har_temperature,
        alpha=args.har_alpha
    )

    # Training loop modifi√© pour passer degree_dict
    ...
else:
    # Focal Loss (actuel)
    criterion = FocalLoss(alpha=args.focal_alpha, gamma=args.focal_gamma)
```

### Changements N√©cessaires

1. **Sampling Strategy** : N√©cessite positive + multiple negatives par anchor
2. **Loss Computation** : Contrastive au lieu de binary classification
3. **Degree Tracking** : Maintenir un dictionnaire de degr√©s √† jour
4. **Hyperparameters** : Tuner temperature, alpha

**Effort d'Impl√©mentation : ~2-3 jours de d√©veloppement + testing**

---

## Peut-on Combiner Focal Loss et HAR ?

**Oui, c'est possible mais complexe.**

### Hybrid Approach

```python
class HybridFocalHARLoss(nn.Module):
    """
    Combine Focal Loss (pour d√©s√©quilibre) et HAR (pour degree bias).
    """

    def __init__(self, focal_gamma=2.0, har_alpha=0.5, lambda_focal=0.5):
        super().__init__()
        self.focal_gamma = focal_gamma
        self.har_alpha = har_alpha
        self.lambda_focal = lambda_focal  # Balance entre focal et HAR

    def forward(self, probs, targets, node_ids, degree_dict):
        # 1. Focal loss component
        p_t = targets * probs + (1 - targets) * (1 - probs)
        focal_weight = (1 - p_t) ** self.focal_gamma
        loss_focal = -focal_weight * torch.log(p_t + 1e-7)

        # 2. HAR degree reweighting
        degrees = torch.tensor([degree_dict[nid] for nid in node_ids])
        har_weight = torch.pow(degrees, -self.har_alpha)

        # 3. Combine
        loss_combined = loss_focal * har_weight
        loss_final = (self.lambda_focal * loss_focal.mean() +
                      (1 - self.lambda_focal) * loss_combined.mean())

        return loss_final
```

**Avantage :** R√©sout les deux probl√®mes (d√©s√©quilibre + degree bias)

**Inconv√©nient :** Encore plus d'hyperparam√®tres, complexit√© accrue

---

## R√©f√©rences Acad√©miques

### Papers Principaux

1. **HAR Contrastive Loss :**
   - Zhang et al. (2021), "Graph Contrastive Learning with Adaptive Augmentation"
   - Wang et al. (2022), "Debiasing Graph Neural Networks via Learning Disentangled Causal Substructure"

2. **Degree Bias in GNNs :**
   - Liu et al. (2021), "Towards Unsupervised Deep Graph Structure Learning"
   - Kang et al. (2022), "Do We Really Need Complicated Model Architectures For Temporal Networks?"

3. **Focal Loss (Votre Approche Actuelle) :**
   - Lin et al. (2017), "Focal Loss for Dense Object Detection"

---

## D√©cision Finale : Flowchart

```
Votre Situation
      |
      v
D√©s√©quilibre extr√™me (0.03%) ?
      |
    [OUI]
      |
      v
Utiliser FOCAL LOSS (Phase 1) ‚úì
      |
      v
Entra√Æner et √©valuer
      |
      v
Degree bias d√©tect√© ?
      |
      +--[NON]-------> Rester avec Focal Loss ‚úì
      |
    [OUI]
      |
      v
Impacter significatif sur votre use case ?
(Besoin de d√©tecter low-degree startups?)
      |
      +--[NON]-------> Rester avec Focal Loss
      |
    [OUI]
      |
      v
Impl√©menter HAR Loss (Phase 2)
      |
      v
Comparer Focal vs HAR
      |
      v
Choisir le meilleur
```

---

## Conclusion et Recommandation

### Pour Votre Cas Sp√©cifique

**Situation Actuelle :**
- D√©s√©quilibre extr√™me (0.03% positifs)
- Focal Loss d√©j√† impl√©ment√© mais pas encore test√©
- Degree bias inconnu

**Recommandation : Approche Progressive** üéØ

```
PHASE 1 (MAINTENANT) :
  ‚úÖ Utiliser Focal Loss
  ‚úÖ √âvaluer performance
  ‚úÖ Diagnostiquer degree bias

PHASE 2 (SI N√âCESSAIRE) :
  ‚è≥ Impl√©menter HAR Loss
  ‚è≥ Comparer avec Focal Loss
  ‚è≥ Choisir la meilleure approche

PHASE 3 (OPTIONNEL) :
  ‚è≥ Hybrid Focal-HAR
  ‚è≥ Fine-tuning
```

### Quick Answer to Your Question

> "HAR contrastive loss pour mitigate degree bias ?"

**Oui**, c'est une excellente approche **SI** :
1. Vous avez un degree bias av√©r√© (√† diagnostiquer d'abord)
2. Vous avez besoin de d√©tecter des low-degree nodes (startups √©mergentes)
3. Vous √™tes pr√™t √† investir dans l'impl√©mentation (~2-3 jours)

**Mais** :
- Commencez avec Focal Loss (d√©j√† fait, r√©sout votre probl√®me principal)
- Diagnostiquez ensuite le degree bias
- Impl√©mentez HAR seulement si vraiment n√©cessaire

---

## Next Steps

Voulez-vous que je :
1. Vous aide √† cr√©er un script de diagnostic de degree bias ?
2. Impl√©mente une version compl√®te de HAR Loss ?
3. Cr√©e un hybrid Focal-HAR Loss ?
4. Analyse vos r√©sultats actuels pour d√©tecter le degree bias ?

